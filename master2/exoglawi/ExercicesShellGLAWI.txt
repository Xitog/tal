
                         _____  _                   _ 
                        |  __ \| |                 (_)
                        | |  \/| |  __ _ __      __ _ 
                        | | __ | | / _` |\ \ /\ / /| |
                        | |_\ \| || (_| | \ V  V / | |
                         \____/|_| \__,_|  \_/\_/  |_|
                              
                              
                                                        Auteur : Damien Gouteux
                                                        Version : 2019-01-22-1a
                                                        
-------------------------------------------------------------------------------
Plan
-------------------------------------------------------------------------------

Table des matières [TAB]
    o Introduction [INTRO]
    o Partie A : utilisation des outils Unix [UNIX]
        o 1. Observation de fichiers volumineux [OBS]
            o 1.1 Décompression de l'extrait de dump du Wiktionnaire
            o 1.2 Observation du fichier décompressé
            o 1.3 Organisation de l'information
        o 2. Manipulations [MAN]
            o 2.1 Extraire la nomenclature
            o 2.2 Extraire la nomenclature en supprimant les pages méta
        o 3. Dump et pages du Wiktionnaire [DUM]
            o 3.1 Extraction d'un article
            o 3.2 Observation de la page en ligne
            o 3.3 Différences dans le wikicode
        4. Analyse et conversion du wikicode [ANA]
            o 4.1 Sections, sous-sections et patrons
                o 4.1.1 Extraction et trie des types de sections
                o 4.1.2 Comptes dans l'extrait
                o 4.1.3 Patrons les plus fréquents
            o 4.2 Langue des étymons
                o 4.2.1 Section étymologie
                o 4.2.2 Extraction des étymologies
                o 4.2.3 Patron des étymologies
                o 4.2.4 Les langues des étymons par fréquence décroissante
            o 4.3 Extraction et conversion des gloses
                o 4.3.1 Signalement des gloses
                o 4.3.2 Extraction et comptage de l'ensemble des gloses
                o 4.3.3 Encodage des hyperliens
                o 4.3.5 Les 10 patrons qui restent
        5. GLÀFF [GLAFF]
        6. Comparaison des nomenclatures de GLÀFF et de Morphalou [MORPH]
        7. Conversion de GLÀFF en format CSV [CSV]
    o Partie B : utilisation de Java [JAVA]

Rechercher par le code rapide pour directement aller à la section voulue.

===============================================================================
Introduction [INTRO]
===============================================================================

Ce document présente le travail fait pour les deux présentations de F. Sajous
sur Glawi dans l'UE Thématiques actuelles de la recherche en traitement 
automatique des langues.

La première partie traite des commandes Unix et la seconde de Java.

===============================================================================
Partie A : utilisation des outils Unix [UNIX]
===============================================================================

Pour cette partie, notre travail a été effectué avec MinGW + MSYS sur Windows.

Nous avons utilisé la partie glawiSplit__1.xml comme base de travail.

Nous avons parfois choisi de décomposer les commandes complexes avec des 
fichiers intermédiaires. Ces fichiers intermédiaires étaient autant de "point de
debug" dans notre mise au point.

1. Observation de fichiers volumineux [OBS]
===========================================

1.1 Décompression de l'extrait de dump du Wiktionnaire
------------------------------------------------------

Nous travaillons sur le premier fichier d'extrait du dump du Wiktionnaire.
Nous le décompressons avec :

..............................
bzip2 -d glawiSplit__1.xml.bz2
..............................

1.2 Observation du fichier décompressé
--------------------------------------

Nous l'observons avec :

......................
less glawiSplit__1.xml
......................

Nous pouvous aussi prendre les 100 premières lignes et les mettre dans un autre
fichier :

......................................................
head -n 100 glawiSplit__1.xml > head_glawiSplit__1.xml
......................................................

1.3 Organisation de l'information
---------------------------------

La balise <page> indique une page qui possède un <titre> et une <revision>.
La révision contient le texte de la page à un moment donné dans la balise 
<text>. Dans les fichiers donnés, il n'y a qu'une révision par page.

Les informations sur le mot sont en WikiCode à l'intérieur de la balise <text> :
    o == {{langue|fr}} == Langue du mot
    o === {{S|nom|fr}} === Catégorie du discours et langue
    o ==== {{S|synonymes}} ==== Synonymes du mot
    o ==== {{S|traductions}} ==== Traductions du mot

2. Manipulations [MAN]
======================

2.1 Extraire la nomenclature
----------------------------

Nous cherchons la balise <title> qui indique une entrée :

............................................
grep \<title\> glawiSplit__1.xml > grep1.txt
............................................

Avec cut :

..............................................................
cut -d ">" -f 2 grep1.txt | cut -d "<" -f 1 > nomenclature.txt
..............................................................

Avec sed :

..........................................................................
sed 's/ *<title>//' grep1.txt | sed 's/<\/title>//' > nomenclature_bis.txt
..........................................................................

On obtient avec les deux méthodes le même fichier (vérifié avec WinMerge).

2.2 Extraire la nomenclature en supprimant les pages méta
---------------------------------------------------------

Les pages méta contiennent un ":" dans le titre.

Nous utilisons un grep inversé pour ne PAS sélectionner les lignes 
qui correspondent au motif :

...............................................................
grep -v " *: *" nomenclature_bis.txt > nomenclature_filtree.txt
...............................................................

La taille de cette nomenclature filtrée est obtenue avec :

..............................
wc -l nomenclature_filtree.txt
..............................

Elle compte 46 337 éléments.

3. Dump et pages du Wiktionnaire [DUM]
======================================

3.1 Extraction d'un article
---------------------------

Pour extraire le contenu de l'article "avant", nous le cherchons avec grep et
nous mettons les 1000 lignes avant et après dans un fichier intermédiaire :

...........................................................................
grep -A 1000 -B 1000 \<title\>avant\</avant\> glawiSplit__1.xml > avant.txt
...........................................................................

À la ligne 1001 du fichier en sortie nous avons bien "    <title>avant</title>".

3.2 Observation de la page en ligne
-----------------------------------

La page en ligne correspondant à ce dump est accessible à cette adresse : https://fr.wiktionary.org/wiki/avant

Comme le Wiktionnaire change constamment, il n'est pas garanti que le dump que
nous avons corresponde exactement à ce qui est actuellement sur la page, 
surtout s'il est ancien.

3.3 Différences dans le wikicode
--------------------------------

Nous avons copié le wikicode du dump qui se trouve dans la balise <text> et 
l'avons comparé au wikicode actuel à l'aide du logiciel WinMerge.
Nous avons donc deux fichiers :
    o wikicode_avant_dump.txt => wikicode de l'entrée "avant" extrait du dump
    o wikicode_avant_2019-01-22-16h41.txt => wikicode de la page à cette date

On peut faire les remarques suivantes à propos du wikicode actuel par rapport 
au dump :
    o Le format du wikicode n'a pas évolué.
    o Des prononciations ont été rajoutées dans la section 
      === {{S|prononciation}} ===
    o Des traductions ont été rajoutées dans la section 
      ==== {{S|traductions}} ==== pour la préposition et l'adverbe français.
    o Une nouvelle entrée pour la langue gallo a été rajoutée 
      (langue d'oïl parlée en Bretagne)
    o Des liens de la forme [[code langue:avant]] ont été supprimés à la fin de
      la page.

Le nombre de '=' indique la profondeur de la section dans l'arbre des sections
du document. On relève la structure suivante dans la version à la date du 22 
janvier 2019 à 16h41 :
    o == {{langue|fr}} ==
        o === {{S|étymologie}} ===
        o === {{S|préposition|fr}} ===
            o ==== {{S|composés}} ====
            o ==== {{S|traductions}} ====
        o === {{S|adverbe|fr}} ===
            o ==== {{S|antonymes}} ====
            o ==== {{S|dérivés}} ====
            o ==== {{S|traductions}} ====
                o ===== {{S|traductions à trier}} =====
        o === {{S|nom|fr}} ===
            o ==== {{S|dérivés}} ====
            o ==== {{S|traductions}} ====
        o === {{S|adjectif|fr}} ===
        o === {{S|prononciation}} ===
            o ==== {{S|homophones}} ====
        o === {{S|anagrammes}} ===
        o === {{S|références}} ===
    o == {{langue|oc}} ==
        o === {{S|étymologie}} ===
        o === {{S|adverbe|oc}} ===
            o ==== {{S|synonymes}} ====
            o ==== {{S|antonymes}} ====
            o ==== {{S|dérivés}} ====
        o === {{S|préposition|oc}} ===
            o ==== {{S|synonymes}} ====
            o ==== {{S|antonymes}} ====
        o === {{S|nom|oc}} ===
            o ==== {{S|synonymes}} ====
            o ==== {{S|dérivés}} ====
        o === {{S|références}} ===

4. Analyse et conversion du wikicode [ANA]
==========================================

4.1 Sections, sous-sections et patrons
--------------------------------------

4.1.1 Extraction et trie des types de sections
----------------------------------------------

Pour trouver les types de section existantes dans le dump, nous utilisons les
commandes suivantes :

.................................................................................................
grep '= {{S|' glawiSplit__1.xml | grep '^=' | grep '=$' | sort > sections.txt
sed -r 's/^=* \{\{S\|//' sections.txt > sections2.txt
sed -r 's/([^\|]*)\|[^|\]+\|[^|\]+\|[^|\]+\}\} ?=+/\1/' sections2.txt | sed -r 's/([^\|]*)\|[^|\]+\|[^|\]+\}\} ?=+/\1/' | sed -r 's/([^\|]*)\|[^|\]+\}\} ?=+/\1/' | sed -r 's/([^\|]*)\}\} ?=+/\1/' | sort | uniq -c | sort -r > types.txt
.................................................................................................

Nous avons préféré faire des étapes intermédiaires.
Dans sections2.txt on se retrouve avec trois types de lignes :
    o type|xxx|xxx|xxx}} =+
    o type|xxx|xxx}} =+
    o type|xxx}} =+
    o type}} =+
Ces trois types de lignes correspondent à nos trois derniers sed.
Nous avons remarqué des séquences incorrectes :
    o absence d'espace entre l'accolade fermante et le premier espace
    o plusieurs espaces entre l'accolade fermante et le premier espace
Nous avons choisi de prendre en compte la première.
La seconde étant trop mineure pour affecter nos résultats.

On compte les doublons avec l'option -c de la commande uniq.
On fait un dernier sort en reverse pour la liste ordonné par fréquences
descendances. Nous reproduisons ici les 5 premières lignes :
    o 62 464 étymologie
    o 46 102 nom
    o 19 985 prononciation
    o 15 359 références
    o 14 136 synonymes
    o 12 996 traductions
    o 12 861 verbe
    o 10 984 adjectif
    o 10 599 voir aussi
    o 10 440 dérivés
L'ensemble des 167 types (dont certains incorrects) se trouve dans le fichier :
types.txt

4.1.2 Comptes dans l'extrait
----------------------------

Nous comptons d'abord les types contenant ' nom ' dans notre extrait :

...........................................
grep -E ' nom( |$)' types.txt > nb_noms.txt
...........................................

Résultats :
    o 46 102 nom
    o  5 218 nom propre
    o     63 nom de famille
    o      6 nom scientifique
    o      5 nom commun
    o      1 nom 
Les types sont au singulier dans notre fichier de résultats.

Le problème de la cohérence est central dans ses ressources. 
Ainsi nom, nom commun, nom et nom scientifique peuvent être agrégés dans un seul
type, nom commun, pour notre exercice. De même, nom propre et nom de famille
dans le type nom propre. Ce qui nous donne :
    o 46 114 noms communs
    o  5 191 noms propres

Nous comptons ensuite les verbes dans notre extrait :

...............................................
grep -E ' verbe( |$)' types.txt > nb_verbes.txt
...............................................

Résultat :
    o 12 861 verbe

Pour compter le nombre d'infinitif et de formes fléchies, il faut revenir à
notre extrait initial. Nous supposons que :
    o Le niveau 2 est une langue
        o === {{S|verbe|fr|flexion}} === : indique une forme fléchie
        o === {{S|verbe|fr}} === : indique un infinitif
        
Le code de langue étant au minimum sur deux caractères en minuscules, il suffit
donc de passer ces commandes :

..................................................................................
grep -E '=== {{S|verbe|[a-z][a-z]+|flexion}} ===' glawiSplit__1.xml > flexions.txt
wc -l flexions.txt
..................................................................................

Ce qui nous donne 2 728 516 formes verbales fléchies.

........................................................................
grep -E '=== {{S|verbe|[a-z][a-z]+}} ===' glawiSplit__1.xml > verbes.txt
wc -l verbes.txt
........................................................................

Ce qui nous donne 271 073 verbes à l'infinitif.

4.1.3 Patrons les plus fréquents
--------------------------------

Pour trouver les patrons les plus fréquents en dehors de ceux indiquant une
section de langue ou un titre de section / sous-section, nous procédons ainsi :

............................................................................................
grep -E '= {{[^{S]' glawiSplit__1.xml | grep -v -E '= {{langue' > patrons.txt
sed -r 's/[^\{]*\{\{([^\}]*)\}\}.*/\1/' patrons.txt > patrons2.txt
sed -r 's/([^\|]*)\|.*$/\1/' patrons2.txt | grep -v -E '*\{?S$' | sort | uniq -c | sort -r > patrons3.txt
............................................................................................

Dans la première ligne, nous sélections toutes les lignes avec un {{ patron }}.
Dans la deuxième ligne, nous ne prenons que l'intérieur des accolades.
Dans la troisième ligne, nous supprimons les arguments du patrons, puis enlevons
les séquences problématiques S et {S avant de trier, de supprimer les doublons
en comptant et enfin de retrier en inverse pour avoir les patrons par ordre 
décroissant de fréquence. Nous avons pour résultats le fichier patrons3.txt,
dont nous prenons les lignes avec une fréquence supérieur ou égale à trois :
    o 625 caractère
    o  35 modl
    o  18 +
    o   8 polytonique
    o   8 fr-conj-1
    o   4 es-conj-2-tener
    o   4 Etymologie graphique chinoise 
    o   3 Étymologie graphique chinoise 
    o   3 es-conj-2-haber
    o   3 de
    o   3 T

Dans notre premier grep, nous ne voulons par des séquences {{S et {{{ car nous
avons remarqué la présence de triplet d'ouverture pour {{{sél, {{{1, {{{nocat
avec la commande suivante :

..................................
grep '{{{' glawiSplit__1.xml | cat
..................................

4.2 Langue des étymons
----------------------

4.2.1 Section étymologie
------------------------

Les étymologies sont dans une section signalée par === {{S|étymologie}} ===
Les lignes de cette section commence toujours par un double point ":".

Voici quelques exemples d'étymologies dans notre extrait autour de l'entrée
avant :

=== {{S|étymologie}} ===
: ''(Préposition et adjectif)'' {{date|842}} Du bas {{étyl|la|fr}} ''[[ab ante#la|ab ante]]'', qui est une forme renforcée de ''[[ante#la|ante]]'' (« avant »).
: ''(Nom)'' {{date|1678}} Même origine. {{date|1422}} ''[[avance]]''.

=== {{S|étymologie}} ===
:De ''[[alternus#la|alternus]]'' avec le suffixe ''[[-e#la|-e]]''.

=== {{S|étymologie}} ===
:{{date}} Mot {{compos|isomère|-ie|lang=fr}}.

4.2.2 Extraction des étymologies
--------------------------------

Pour extraire les étymmologies, nous prenons toutes les lignes commençant par
un double point.

.............................................
grep '^:' glawiSplit__1.xml > etymologies.txt
.............................................

4.2.3 Patron des étymologies
----------------------------

Le patron utilisé est : {{étyl}}.
Il est documenté sur cette page :
    https://fr.wiktionary.org/wiki/Mod%C3%A8le:%C3%A9tyl
    
Sa syntaxe :
    {{étyl|
        code-langue-1|
        code-langue-2|
        mot=mot1|
        tr=translittération1|
        type=code-gramm|
        num=lemme|
        sens=traduction1
    }}

Quelques exemples :
    o {{étyl|la|fr|mot=alternus}} pour alterne
    o {{étyl|la|nl|mot=albumen|sens=blanc d’œuf}} pour albumen

Le code-langue-1 indique la langue d'origine de l'étymon.
Le code-langue-2 indique la langue de l'entrée.
    On peut s'interroger sur le "nl" du deuxième exemple qui pour nous est une
    erreur, il devrait être mis "fr". Cela a été corrigée dans la version en
    ligne d'albumen (au 23 janvier 2019 à 16h29).

4.2.4 Les langues des étymons par fréquence décroissante
--------------------------------------------------------

Pour extraire toutes les utilisations du patron, nous utilisons les commandes suivantes :

.............................................................................................
grep "tyl|" glawiSplit__1.xml | sed -r 's/[^\{]*\{\{[^\|]*([^\}]*)\}.*/\1/' > etymologies_patrons.txt
.............................................................................................

Le fichier etymologies_patrons.txt contient seulement les arguments du patron car nous
avions un problème d'encodage sur le 'é' de étyl. Nous avons contourné la
difficulté dans l'écriture de notre expression régulière. Voici des exemples
tirés du fichier :

|it|fr|mot=lira
|la|fr|mot=interrogatio
|la|fr|mot=procrastino|dif=procrastinare

Nous faisons ensuite une extraction du code-langue-1 avec trie et unicité pour
obtenir une liste des langues d'étymons par ordre décroissant de fréquence :

..................................................................................
sed -r 's/^\|([^|]*)\|.*/\1/' etymologies_patrons.txt | sort | uniq -c | sort -r > langues_etymons.txt
..................................................................................

Les 10 langues d'origine des étymons les plus fréquentes sont :
   o 8 894 la
   o 1 025 fr
   o   848 grc
   o   619 en
   o   406 it
   o   365 de
   o   337 indo-européen commun
   o   333 es
   o   320 eo
   o   304 goh

4.3 Extraction et conversion des gloses
---------------------------------------

4.3.1 Signalement des gloses
----------------------------

Nous avons étudié l'entrée "tour" car elle contient deux sens :
    o la tour, construction défensive élevée
    o le tour, déplacement qui finit là où il commence
    
Les deux gloses / sens sont traduits par deux sections :
    o === {{S|nom|fr|num=1}} ===
    o === {{S|nom|fr|num=2}} ===

Les gloses sont donc des sections de la forme : 

=== {{S|pos|code-langue|num=nombre}} ===

Le nombre est là pour distinguer entre plusieurs sens de même catégorie du
discours (part of speech).

4.3.2 Extraction et comptage de l'ensemble des gloses
-----------------------------------------------------

Selon la page : 
https://fr.wiktionary.org/wiki/Wiktionnaire:Liste_de_tous_les_mod%C3%A8les/Titres_de_sections/Liste_automatique

Il existe 67 catégories du discours différentes, certaines utilisant le même
code. Nous avons fait le fichier pos.txt qui reprend ces catégories par un 
copier-coller de la page. Pour que cela marche avec cut et uniq, avec Notepad++,
nous convertissons les tabulations en espace et les fins de ligne en UNIX.

Nous exécutons ensuite la commande suivante :

....................................................
cut -d ' ' -f 1 pos.txt | sort | uniq > pos_uniq.txt
....................................................

Pour obtenir dans le fichier pos_uniq.txt juste les codes uniques. 

Puis, par concaténation manuelle nous fabriquons la commande suivante :

................................................................................
grep -E "{{S\|adj|{{S\|adj-dém|{{S\|adj-excl|{{S\|adj-indéf|{{S\|adj-int|{{S\|adj-num|{{S\|adj-pos|{{S\|adj-rel|{{S\|adjectif|{{S\|adv|{{S\|adv-ind|{{S\|adv-int|{{S\|adv-pron|{{S\|adv-rel|{{S\|adverbe|{{S\|aff|{{S\|art|{{S\|art-déf|{{S\|art-indéf|{{S\|art-part|{{S\|article|{{S\|circon|{{S\|circonf|{{S\|class|{{S\|classif|{{S\|conj|{{S\|conj-coord|{{S\|conjonction|{{S\|copule|{{S\|dét|{{S\|encl|{{S\|faute|{{S\|gismu|{{S\|inf|{{S\|interf|{{S\|interj|{{S\|lettre|{{S\|loc|{{S\|loc-phr|{{S\|locution|{{S\|locution-phrase|{{S\|nom|{{S\|nom-fam|{{S\|nom-pr|{{S\|nom-sciences|{{S\|num|{{S\|numeral|{{S\|numér|{{S\|onom|{{S\|onoma|{{S\|part|{{S\|part-num|{{S\|particule|{{S\|patronyme|{{S\|phr|{{S\|phrase|{{S\|post|{{S\|postpos|{{S\|procl|{{S\|pronom|{{S\|pronom-adj|{{S\|pronom-dém|{{S\|pronom-indéf|{{S\|pronom-int|{{S\|pronom-per|{{S\|pronom-pers|{{S\|pronom-pos|{{S\|pronom-rel|{{S\|pronom-réfl|{{S\|prov|{{S\|pré-nom|{{S\|pré-verb|{{S\|préf|{{S\|prénom|{{S\|prép|{{S\|quantif|{{S\|rad|{{S\|radical|{{S\|rafsi|{{S\|sino|{{S\|sinog|{{S\|sinogramme|{{S\|substantif|{{S\|suf|{{S\|suff|{{S\|symb|{{S\|var-typo|{{S\|variante|{{S\|verb" glawiSplit__1.xml > gloses1.txt
................................................................................

Nous avons ainsi toutes les patrons indiquant une glose avec ses paramètres.

Pour faire un compte, il suffit d'effectuer :

.....................................................................................
cut -d '|' -f 2 gloses1.txt | sed -r 's/^([a-z]*).*/\1/' | sort | uniq -c | sort -r > gloses_count.txt
.....................................................................................

Les 5 premières lignes du fichier gloses_count.txt sont :
    o 52 313 nom
    o 13 137 verbe
    o 12 050 adjectif
    o  3 608 adverbe
    o  2 857 variantes

4.3.3 Encodage des hyperliens
-----------------------------

Les hyperliens sont encodés en wikicode ainsi : [[ ]]

Ils peuvent eux aussi avec des arguments à l'intérieur, séparés par |

Les liens vers des fichiers ont "Fichier:" comme préfixe :

[[Fichier:Tour de Montlhéry.jpg|vignette|La '''tour''' de Montlhéry ''(sens 1)'']]

...............................................................................................
sed -r 's/\[\[(Fichier|Image):.*\]\]//' glawiSplit__1.xml | sed -r 's/\[\[[^\:]*:(.*)\]\]/\1/g' | sed -r 's/\[\[([^\|]*).*\]\]/\1/' | sed -r 's/\[\[//' | sed -r 's/\]\]//' > nohyperlink.txt
...............................................................................................

Notes :
    o Le premier sed remplace les liens de la forme "[[Fichier:...]" ou ceux
      de la forme "[[Image:...]]" par rien.
    o Le deuxième sed remplace les liens de la forme [[langue:mot]] par mot.
    o Le troisième sed remplace les liens de la forme [[xxx|...]] par xxx.
    o Si sur une même ligne on a "[[xxx]] ou [[yyy]]" le troisième sed donne
      "xxx]]" ou "[[yyy". Pour effacer les dernières traces on fait deux simples
      sed sur "]]" et "[[".
    o Théoriquement, en rendant l'opérateur * non greedy avec ?, cela devrait
      éviter de faire les deux derniers sed, mais sur notre environnement, cela
      ne marche pas. Après bien des interrogations, nous avons essayé l'exemple
      suivant donné dans le cours :
      ..............................................................
      echo "un lapin au vin blanc" | sed -r 's/l.*?n/DICTIONNAIRE/g'
      ..............................................................
      Malgré l'opérateur ? modificateur du * pour le rendre non greedy
      et le modificateur global g pour effectuer le remplacement autant de fois 
      que le motif est trouvé, nous obtenons : un DICTIONNAIREc au lieu de :
      un DICTIONNAIRE au vin bDICTIONNAIREc
    o Nous avons également testé avec le même résultat ici (GNU Sed 4.4) :
      https://www.tutorialspoint.com/execute_bash_online.php

Une solution serait d'utiliser Perl pour palier le problème de .*? :

...............................................................................................
perl -CSDA -p -e 's/\[\[(Fichier|Image):.*\]\]//' glawiSplit__1.xml | perl -CDSA  -p -e 's/\[\[[^\:]*:(.*)\]\]/\1/g' | perl -CSDA -p -e 's/\[\[([^#]*?)\#.*?\]\]/\1/g' | perl -CSDA -p -e 's/\[\[([^\|]*?)\|.*?\]\]/\1/g' | perl -CSDA -p -e 's/\[\[(.*?)\]\]/\1/g' | perl -CDSA -p -e 's/(\[\[)|(\]\])//g' > nohyperlink2.txt
...............................................................................................

Cette dernière commande permet de corriger de nombreux problèmes de la
précédente.

4.3.4 Encodage du gras et de l'italique
---------------------------------------

Le gras est encodé par '''mot en gras'''.
L'italique est encodé par ''mot en italique''.

On fait l'opération suivante :

...................................................
sed -r "s/'''?//g" glawiSplit__1.xml > noquotes.txt
...................................................

Pour encoder le formatage au lieu de le supprimer :

..........................................................................................
sed -r "s/'''(.*?)'''/<b>\1<\/b>/g" glawiSplit__1.xml| sed -r "s/''(.*?)''/<i>\1<\/i>/g" > noquotes.txt
..........................................................................................

Notes : comme expliqué plus haut, utiliser le ? pour rendre le * non greedy ne
marche pas sur ma configuration. Il le faut car sinon :
''aaa'' ''bbb'' est transformé en <i>aaa'' ''bbb</i> car .* "mange" le plus
possible. Alors qu'avec .?*, il s'arrête au premier '' qu'il rencontre ce qui
est essentiel pour différencier un '' fermant d'un '' ouvrant pour remplacer par
<i> ou <\i>.

Une solution est d'utiliser Perl avec :
................................................................................
perl -p -e "s/'''(.*?)'''/<b>\1<\/b>/g" glawiSplit__1.xml | perl -p -e "s/''(.*?)''/<i>\1<\/i>/g" > quotebold.txt
................................................................................

Mais nous avons un autre problème : la succession de deux balises wikicode comme
dans l'exemple :

#* '''''Pendant''' de baudrier ou de ceinturon.''

La bonne traduction est <i><b>Pendant</b> de baudrier ou de ceinturon.</i>
Alors que la nôtre est <b><i>Pendant</b> de baudrier ou de ceinturon.</i>
Ce qui ne respecte pas les règles XHTML (HTML est lui permissif) :
On ne doit pas fermer une balise parente puis fermer une balise fille.
Cela vient du fait que nous matchons en premier le triple. On corrige ainsi
notre commande :

.................................................................................
perl -CSDA -p -e "s/'''([\w ].*?)'''/<b>\1<\/b>/g" glawiSplit__1.xml | perl -p -e "s/''(.*?)''/<i>\1<\/i>/g" > quotebold.txt
.................................................................................

Nous obtenons la bonne traduction. Nous précisons CSDA pour signaler à Perl de
fonctionner en Unicode pour que \w reconnaisse bien toutes les lettres non
romanes.

4.3.5 Les 10 patrons qui restent
--------------------------------

Nous cumulons les deux traitements précédents pour obtenir un fichier sans
liens et en ayant converti le gras et l'italique.

...............................................................................................
perl -CSDA -p -e 's/\[\[(Fichier|Image):.*\]\]//' glawiSplit__1.xml | perl -CDSA  -p -e 's/\[\[[^\:]*:(.*)\]\]/\1/g' | perl -CSDA -p -e 's/\[\[([^#]*?)\#.*?\]\]/\1/g' | perl -CSDA -p -e 's/\[\[([^\|]*?)\|.*?\]\]/\1/g' | perl -CSDA -p -e 's/\[\[(.*?)\]\]/\1/g' | perl -CDSA -p -e 's/(\[\[)|(\]\])//g' | perl -CSDA -p -e "s/'''([\w ].*?)'''/<b>\1<\/b>/g" | perl -p -e "s/''(.*?)''/<i>\1<\/i>/g" > nothing.txt
...............................................................................................

Il faut à présent sélectionner les patrons en enlevant tous les patrons traitant
des types de mots déjà analysés :

................................................................................
grep -E "{{" nothing.txt | grep -v -E "{{S\|adj|{{S\|adj-dém|{{S\|adj-excl|{{S\|adj-indéf|{{S\|adj-int|{{S\|adj-num|{{S\|adj-pos|{{S\|adj-rel|{{S\|adjectif|{{S\|adv|{{S\|adv-ind|{{S\|adv-int|{{S\|adv-pron|{{S\|adv-rel|{{S\|adverbe|{{S\|aff|{{S\|art|{{S\|art-déf|{{S\|art-indéf|{{S\|art-part|{{S\|article|{{S\|circon|{{S\|circonf|{{S\|class|{{S\|classif|{{S\|conj|{{S\|conj-coord|{{S\|conjonction|{{S\|copule|{{S\|dét|{{S\|encl|{{S\|faute|{{S\|gismu|{{S\|inf|{{S\|interf|{{S\|interj|{{S\|lettre|{{S\|loc|{{S\|loc-phr|{{S\|locution|{{S\|locution-phrase|{{S\|nom|{{S\|nom-fam|{{S\|nom-pr|{{S\|nom-sciences|{{S\|num|{{S\|numeral|{{S\|numér|{{S\|onom|{{S\|onoma|{{S\|part|{{S\|part-num|{{S\|particule|{{S\|patronyme|{{S\|phr|{{S\|phrase|{{S\|post|{{S\|postpos|{{S\|procl|{{S\|pronom|{{S\|pronom-adj|{{S\|pronom-dém|{{S\|pronom-indéf|{{S\|pronom-int|{{S\|pronom-per|{{S\|pronom-pers|{{S\|pronom-pos|{{S\|pronom-rel|{{S\|pronom-réfl|{{S\|prov|{{S\|pré-nom|{{S\|pré-verb|{{S\|préf|{{S\|prénom|{{S\|prép|{{S\|quantif|{{S\|rad|{{S\|radical|{{S\|rafsi|{{S\|sino|{{S\|sinog|{{S\|sinogramme|{{S\|substantif|{{S\|suf|{{S\|suff|{{S\|symb|{{S\|var-typo|{{S\|variante|{{S\|verb" > nothing2.txt
................................................................................
 
A partir du fichier nothing2.txt obtenu, nous analysons les patrons restant :

...............................................................................................
perl -CSDA -p -e 's/(.*?)\{\{(.*?)\}\}(.*?)/\2/g' nothing2.txt > nothing3.txt

sort nothing3.txt | uniq -c | sort -r > nothing4.txt

head -n 10 nothing4.txt > last_patrons.txt
...............................................................................................

Voici les 10 patrons les plus fréquents restant :
    o 62 456 S|étymologie ===
    o 19 976 S|prononciation ===
    o 17 225 langue|fr ==
    o 16 420 trad-fin
    o 15 273 S|références ===
    o 14 115 S|synonymes ====
    o 12 881 S|traductions ====
    o 10 589 S|voir aussi ===
    o 10 386 S|dérivés ====
    o  9 129 trad-début
J'ai laissé les "=" finaux pour analyser le niveau de titre.

Nous avons observés ces dix patrons à l'aide de grep par exemple pour trois
d'entre eux :

.............................................
grep -E "\{\{S\|étymologie" glawiSplit__1.xml
grep -E trad-fin avant.txt | head -n 10
grep -E trad-début avant.txt | head -n 10
.............................................

À partir de ces observations, nous avons construit des commandes pour remplacer
les 10 patrons les plus fréquents pour rendre plus lisible le Wikicode.

Nous avons notamment choisi de traduire :
{{trad-début|Composés chimique en chimie (1)}}
    xxx
{{trad-fin}}
Par :
<p><u>Composés chimique en chimie (1)</u> :
    xxx
</p>

À chaque ligne suivante correspond un changement de patron :

................................................................................................
perl -CSDA -p -e 's/=== \{\{S\|.tymologie\}\} ===/<h3>Etymologie<\/h3>/g' glawiSplit__1.xml | \
perl -CSDA -p -e 's/=== \{\{S\|prononciation\}\} ===/<h3>Prononciation<\/h3>/g' | \
perl -CSDA -p -e 's/=== \{\{S\|r.f.rences\}\} ===/<h3>References<\/h3>/g' | \
perl -CSDA -p -e 's/=== \{\{S\|synonymes\}\} ===/<h3>Synonymes<\/h3>/g' | \
perl -CSDA -p -e 's/=== \{\{S\|traductions\}\} ===/<h3>Traductions<\/h3>/g' | \
perl -CSDA -p -e 's/=== \{\{S\|voir aussi\}\} ===/<h3>Voir aussi<\/h3>/g' | \
perl -CSDA -p -e 's/=== \{\{S\|d.riv.s\}\} ===/<h3>Derives<\/h3>/g' | \
perl -CSDA -p -e 's/\{\{trad-d.but\|(.*?)\}\}/<p><u>\1<\/u> :/g' | \
perl -CSDA -p -e 's/\{\{trad-fin\}\}/<\/p>/g'> last10.txt
................................................................................................

Nous avons dû utiliser le "." à chaque caractère accentué, ayant des problèmes
d'encodage.

5. GLÀFF [GLAFF]
================

6. Comparaison des nomenclatures de GLÀFF et de Morphalou [MORPH]
=================================================================

7. Conversion de GLÀFF en format CSV [CSV]
==========================================

-------------------------------------------------------------------------------
Partie B : utilisation de Java [JAVA]
-------------------------------------------------------------------------------
