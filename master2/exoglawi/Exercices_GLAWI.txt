
                         _____  _                   _ 
                        |  __ \| |                 (_)
                        | |  \/| |  __ _ __      __ _ 
                        | | __ | | / _` |\ \ /\ / /| |
                        | |_\ \| || (_| | \ V  V / | |
                         \____/|_| \__,_|  \_/\_/  |_|
                              
                              
                                                        Auteur : Damien Gouteux
                                                        Version : 2019-01-26-1a
                                                        
--------------------------------------------------------------------------------
Plan
--------------------------------------------------------------------------------

Table des matières [TAB]
    o Introduction [INTRO]
    o Partie A : utilisation des outils Unix [UNIX]
        o 1. Observation de fichiers volumineux [OBS]
            o 1.1 Décompression de l'extrait de dump du Wiktionnaire
            o 1.2 Observation du fichier décompressé
            o 1.3 Organisation de l'information
        o 2. Manipulations [MAN]
            o 2.1 Extraire la nomenclature
            o 2.2 Extraire la nomenclature en supprimant les pages méta
        o 3. Dump et pages du Wiktionnaire [DUM]
            o 3.1 Extraction d'un article
            o 3.2 Observation de la page en ligne
            o 3.3 Différences dans le wikicode
        4. Analyse et conversion du wikicode [ANA]
            o 4.1 Sections, sous-sections et patrons
                o 4.1.1 Extraction et trie des types de sections
                o 4.1.2 Comptes dans l'extrait
                o 4.1.3 Patrons les plus fréquents
            o 4.2 Langue des étymons
                o 4.2.1 Section étymologie
                o 4.2.2 Extraction des étymologies
                o 4.2.3 Patron des étymologies
                o 4.2.4 Les langues des étymons par fréquence décroissante
            o 4.3 Extraction et conversion des gloses
                o 4.3.1 Signalement des gloses
                o 4.3.2 Extraction et comptage de l'ensemble des gloses
                o 4.3.3 Encodage des hyperliens
                o 4.3.5 Les 10 patrons qui restent
        5. GLÀFF [GLÀFF]
            o 5.1 Nombre de formes, nombre de lemmes
            o 5.2 Trie des étiquettes morphosyntaxiques par fréquence
                  décroissante
            o 5.3 Formes fléchis du verbe googler
            o 5.4 Formes fléchis avec étiquettes morphosyntaxiques du verbe
                  hacker
            o 5.5 Nombre de formes fléchies pour chaque verbe 
            o 5.6 Verbes surabondants et verbes défectifs
        6. Comparaison des nomenclatures de GLÀFF et de Morphalou [MORPH]
            o 6.1 Liste des formes dans le Morphalou et pas dans GLÀFF
            o 6.2 Liste des formes dans GLÀFF et pas dans Morphalou
        7. Conversion de GLÀFF en format CSV [CSV]
            o 7.1 Conversion du GLÀFF en CSV
    o Partie B : utilisation de Java [JAVA]
        1. Environnement de travail [ENV]
        2. Extractions à partir du GLAWI [EXT]
            o 2.1 Extraction de la nomenclature
            o 2.2 Table de fréquences des parties du discours
            o 2.3 Types de marques lexicographiques
            o 2.4 Les 5 marques de domaine les plus fréquents
            o 2.5a Entrées avec au moins une définition dans le domaine le plus
                   fréquent
            o 2.5b Entrées avec toutes les définitions dans le domaine le plus
                   fréquent
            o 2.6 Extraction des entrées avec leurs gloses qui sont des 
                  néologies
            o 2.7 Extraction des entrées avec plusieurs gloses dont une seule
                  est une néologie
            o 2.8 Extraction des entrées importées du Littré
            o 2.9 Extraction des entrées importées du Littré portant une marque 
                  de néologie
            o 2.10 Extraction des entrées importées du Littré dont toutes les
                   gloses sont des néologies
        3. Inspecteur de structure [INS]
[FIN]

Rechercher par le code rapide entre crochets pour directement aller à la section
voulue.

================================================================================
Introduction [INTRO]
================================================================================

Ce document présente le travail fait pour les deux présentations de F. Sajous
sur Glawi dans l'UE Thématiques actuelles de la recherche en traitement 
automatique des langues.

La première partie traite des commandes Unix, la seconde partie traite de Java.

Les fichiers fournis sont :
    o Exercices_GLAWI.txt     ce fichier
    o saxo/DGXHandler.java    code Java pour la partie 2, question 2
    o saxo/Inspector.java     code Java pour la partie 2, question 3
    o partie1/                certains fichiers obtenus pour la partie 1
    o partie2/                tous les fichiers obtenus pour la partie 2
    
Les réponses sont reproduites directement dans ce fichier si elle n'était pas
trop volumineuse. 

Le code Java a été documenté.

Ce fichier a été écrit pour être lu sur un terminal avec des lignes de 80
caractères sauf pour les commandes que nous avons laissées verbatim.

================================================================================
Partie A : utilisation des outils Unix [UNIX]
================================================================================

Pour cette partie, notre travail a été effectué avec MinGW + MSYS sur Windows.

Nous avons utilisé le fichier glawiSplit__1.xml comme base de travail.

Nous avons parfois choisi de décomposer les commandes complexes avec des 
fichiers intermédiaires. Ces fichiers intermédiaires étaient autant de "point de
debug" dans notre mise au point. Nous n'avons pas cherché à optimiser les 
commandes.

Tous les fichiers sont reproductibles en suivant les commandes décrites ici.
Nous avons également mis dans le dossier partie1 quelques fichiers émis
par nos commandes.

Toutes les commandes sont signalées par des lignes de points.

1. Observation de fichiers volumineux [OBS]
===========================================

1.1 Décompression de l'extrait de dump du Wiktionnaire
------------------------------------------------------

Nous travaillons sur le premier fichier d'extrait du dump du Wiktionnaire 
glawiSplit__1.xml. Nous le décompressons avec :

..............................
bzip2 -d glawiSplit__1.xml.bz2
..............................

1.2 Observation du fichier décompressé
--------------------------------------

Nous l'observons avec :

......................
less glawiSplit__1.xml
......................

Nous pouvous aussi prendre les 100 premières lignes et les mettre dans un autre
fichier :

......................................................
head -n 100 glawiSplit__1.xml > head_glawiSplit__1.xml
......................................................

1.3 Organisation de l'information
---------------------------------

La balise <page> indique une page qui possède un <titre> et une <revision>.
La révision contient le texte de la page à un moment donné dans la balise 
<text>. Dans les fichiers donnés, il n'y a qu'une révision par page.

Les informations sur le mot sont en WikiCode à l'intérieur de la balise <text> :
    o == {{langue|fr}} == Langue du mot
    o === {{S|nom|fr}} === Catégorie du discours et langue
    o ==== {{S|synonymes}} ==== Synonymes du mot
    o ==== {{S|traductions}} ==== Traductions du mot

2. Manipulations [MAN]
======================

2.1 Extraire la nomenclature
----------------------------

Nous cherchons la balise <title> qui indique une entrée :

............................................
grep \<title\> glawiSplit__1.xml > grep1.txt
............................................

ou 
............................................
grep "<title>" glawiSplit__1.xml > grep1.txt
............................................

Avec cut :

..................................................................
cut -d ">" -f 2 grep1.txt | cut -d "<" -f 1 > 2_1_nomenclature.txt
..................................................................

Avec sed :

..............................................................................
sed 's/ *<title>//' grep1.txt | sed 's/<\/title>//' > 2_1_nomenclature_bis.txt
..............................................................................

On obtient avec les deux méthodes le même fichier, qui compte 48 379 titres.
Nous avons vérifié la correspondance avec diff et le logiciel WinMerge.

2.2 Extraire la nomenclature en supprimant les pages méta
---------------------------------------------------------

Les pages méta contiennent un ":" dans le titre.

Nous utilisons un grep inversé pour ne PAS sélectionner les lignes 
qui correspondent au motif :

.......................................................................
grep -v " *: *" 2_1_nomenclature_bis.txt > 2_2_nomenclature_filtree.txt
.......................................................................

Sans l'option -E nous pouvons toujours utilisé une expression régulière basique
comme patron.

La taille de cette nomenclature filtrée est obtenue avec :

..................................
wc -l 2_2_nomenclature_filtree.txt
..................................

Elle compte 46 337 éléments.

3. Dump et pages du Wiktionnaire [DUM]
======================================

3.1 Extraction d'un article
---------------------------

Pour extraire le contenu de l'article "avant", nous le cherchons avec grep et
nous mettons les 1000 lignes avant et après dans un fichier intermédiaire :

...........................................................................
grep -A 1000 -B 1000 \<title\>avant\</avant\> glawiSplit__1.xml > avant.txt
...........................................................................

À la ligne 1001 du fichier en sortie nous avons bien "    <title>avant</title>".

3.2 Observation de la page en ligne
-----------------------------------

La page en ligne correspondant à ce dump est accessible à cette adresse :
https://fr.wiktionary.org/wiki/avant

Comme le Wiktionnaire change constamment, il n'est pas garanti que le dump que
nous avons corresponde exactement à ce qui est actuellement sur la page, 
surtout s'il est ancien.

3.3 Différences dans le wikicode
--------------------------------

Nous avons copié le wikicode du dump qui se trouve dans la balise <text> et 
l'avons comparé au wikicode actuel à l'aide du logiciel WinMerge.
Nous avons donc deux fichiers :
    o wikicode_avant_dump.txt => wikicode de l'entrée "avant" extrait du dump
    o wikicode_avant_2019-01-22-16h41.txt => wikicode de la page à cette date

On peut faire les remarques suivantes à propos du wikicode actuel par rapport 
au dump :
    o Le format du wikicode n'a pas évolué.
    o Des prononciations ont été rajoutées dans la section 
      === {{S|prononciation}} ===
    o Des traductions ont été rajoutées dans la section 
      ==== {{S|traductions}} ==== pour la préposition et l'adverbe français.
    o Une nouvelle entrée pour la langue gallo a été rajoutée 
      (langue d'oïl parlée en Bretagne)
    o Des liens de la forme [[code langue:avant]] ont été supprimés à la fin de
      la page.

Le nombre de '=' indique la profondeur de la section dans l'arbre des sections
du document. On relève la structure suivante dans la version à la date du 22 
janvier 2019 à 16h41 :
    o == {{langue|fr}} ==
        o === {{S|étymologie}} ===
        o === {{S|préposition|fr}} ===
            o ==== {{S|composés}} ====
            o ==== {{S|traductions}} ====
        o === {{S|adverbe|fr}} ===
            o ==== {{S|antonymes}} ====
            o ==== {{S|dérivés}} ====
            o ==== {{S|traductions}} ====
                o ===== {{S|traductions à trier}} =====
        o === {{S|nom|fr}} ===
            o ==== {{S|dérivés}} ====
            o ==== {{S|traductions}} ====
        o === {{S|adjectif|fr}} ===
        o === {{S|prononciation}} ===
            o ==== {{S|homophones}} ====
        o === {{S|anagrammes}} ===
        o === {{S|références}} ===
    o == {{langue|oc}} ==
        o === {{S|étymologie}} ===
        o === {{S|adverbe|oc}} ===
            o ==== {{S|synonymes}} ====
            o ==== {{S|antonymes}} ====
            o ==== {{S|dérivés}} ====
        o === {{S|préposition|oc}} ===
            o ==== {{S|synonymes}} ====
            o ==== {{S|antonymes}} ====
        o === {{S|nom|oc}} ===
            o ==== {{S|synonymes}} ====
            o ==== {{S|dérivés}} ====
        o === {{S|références}} ===

4. Analyse et conversion du wikicode [ANA]
==========================================

4.1 Sections, sous-sections et patrons
--------------------------------------

4.1.1 Extraction et trie des types de sections
----------------------------------------------

Pour trouver les types de section existantes dans le dump, nous utilisons les
commandes suivantes :

................................................................................
grep '= {{S|' glawiSplit__1.xml | grep '^=' | grep '=$' | sort > sections.txt
sed -r 's/^=* \{\{S\|//' sections.txt > sections2.txt
sed -r 's/([^\|]*)\|[^|\]+\|[^|\]+\|[^|\]+\}\} ?=+/\1/' sections2.txt | sed -r 's/([^\|]*)\|[^|\]+\|[^|\]+\}\} ?=+/\1/' | sed -r 's/([^\|]*)\|[^|\]+\}\} ?=+/\1/' | sed -r 's/([^\|]*)\}\} ?=+/\1/' | sort | uniq -c | sort -r > 4_1_1_types.txt
................................................................................

Nous avons préféré faire des étapes intermédiaires.
Dans sections2.txt on se retrouve avec trois types de lignes :
    o type|xxx|xxx|xxx}} =+
    o type|xxx|xxx}} =+
    o type|xxx}} =+
    o type}} =+
Ces trois types de lignes correspondent à nos trois derniers sed.
Nous avons remarqué des séquences incorrectes :
    o absence d'espace entre l'accolade fermante et le premier espace
    o plusieurs espaces entre l'accolade fermante et le premier espace
Nous avons choisi de prendre en compte la première.
La seconde étant trop mineure pour affecter nos résultats.

On compte les doublons avec l'option -c de la commande uniq.
On fait un dernier sort en reverse pour la liste ordonné par fréquences
descendances. Nous reproduisons ici les 5 premières lignes :
    o 62 464 étymologie
    o 46 102 nom
    o 19 985 prononciation
    o 15 359 références
    o 14 136 synonymes
    o 12 996 traductions
    o 12 861 verbe
    o 10 984 adjectif
    o 10 599 voir aussi
    o 10 440 dérivés
L'ensemble des 167 types (dont certains incorrects) se trouve dans le fichier :
types.txt

4.1.2 Comptes dans l'extrait
----------------------------

Nous comptons d'abord les types contenant ' nom ' dans notre extrait :

.................................................
grep -E ' nom( |$)' 4_1_1_types.txt > nb_noms.txt
.................................................

Résultats :
    o 46 102 nom
    o  5 218 nom propre
    o     63 nom de famille
    o      6 nom scientifique
    o      5 nom commun
    o      1 nom 
Les types sont au singulier dans notre fichier de résultats.

Le problème de la cohérence est central dans ses ressources. 
Ainsi nom, nom commun, nom et nom scientifique peuvent être agrégés dans un seul
type, nom commun, pour notre exercice. De même, nom propre et nom de famille
dans le type nom propre. Ce qui nous donne :
    o 46 114 noms communs
    o  5 191 noms propres

Nous comptons ensuite les verbes dans notre extrait :

.....................................................
grep -E ' verbe( |$)' 4_1_1_types.txt > nb_verbes.txt
.....................................................

Résultat :
    o 12 861 verbe

Pour compter le nombre d'infinitif et de formes fléchies, il faut revenir à
notre extrait initial. Nous supposons que :
    o Le niveau 2 est une langue
        o === {{S|verbe|fr|flexion}} === : indique une forme fléchie
        o === {{S|verbe|fr}} === : indique un infinitif
        
Le code de langue étant au minimum sur deux caractères en minuscules, il suffit
donc de passer ces commandes :

................................................................................
grep -E '=== {{S|verbe|[a-z][a-z]+|flexion}} ===' glawiSplit__1.xml > flexions.txt
wc -l flexions.txt
................................................................................

Ce qui nous donne 2 728 516 formes verbales fléchies.

........................................................................
grep -E '=== {{S|verbe|[a-z][a-z]+}} ===' glawiSplit__1.xml > verbes.txt
wc -l verbes.txt
........................................................................

Ce qui nous donne 271 073 verbes à l'infinitif.

4.1.3 Patrons les plus fréquents
--------------------------------

Pour trouver les patrons les plus fréquents en dehors de ceux indiquant une
section de langue ou un titre de section / sous-section, nous procédons ainsi :

1) Nous prenons toutes les lignes avec '{{' :

............................................
grep -E '{{' glawiSplit__1.xml > patrons.txt
............................................

2) Nous extrayons tous les patrons en en mettant 1 par ligne :

...................................................................
sed -r 's/[^\{]*\{\{([^\}]*)\}\}.*/\1/g' patrons.txt > patrons2.txt
...................................................................

Il faut faire attention de mettre le modifieur global g pour sed car une même
ligne peut contenir plusieurs patrons.

3) Nous supprimons les arguments :

.........................................................
sed -r 's/([^\|]*)\|?.*$/\1/' patrons2.txt > patrons3.txt
.........................................................

4) Nous comptons en triant et unifiant en comptant, en retriant selon la
fréquence décroissante et en supprimant "langue" et "S" :

....................................................................................
sort patrons3.txt | grep -v -E '^(langue|S)$ | uniq -c | sort -r > 4_3_3_patrons.txt
....................................................................................

Nous avons pour résultats le fichier 4_3_3_patrons.txt, dont nous reproduisons
ci-dessous les dix premières lignes :
    o 140 100 T
    o  73 191 pron
    o  31 688 ébauche-étym
    o  23 631 source
    o  16 462 trad-fin
    o  16 446 trad-début
    o  15 874 étyl
    o  13 855 lien
    o   9 437 écouter
    o   9 060 clé de tri

4.2 Langue des étymons
----------------------

4.2.1 Section étymologie
------------------------

Les étymologies sont dans une section signalée par === {{S|étymologie}} ===
Les lignes de cette section commence toujours par un double point ":".

Voici quelques exemples d'étymologies dans notre extrait autour de l'entrée
avant :

=== {{S|étymologie}} ===
: ''(Préposition et adjectif)'' {{date|842}} Du bas {{étyl|la|fr}} ''[[ab ante#la|ab ante]]'', qui est une forme renforcée de ''[[ante#la|ante]]'' (« avant »).
: ''(Nom)'' {{date|1678}} Même origine. {{date|1422}} ''[[avance]]''.

=== {{S|étymologie}} ===
:De ''[[alternus#la|alternus]]'' avec le suffixe ''[[-e#la|-e]]''.

=== {{S|étymologie}} ===
:{{date}} Mot {{compos|isomère|-ie|lang=fr}}.

4.2.2 Extraction des étymologies
--------------------------------

Pour extraire les étymmologies, nous prenons toutes les lignes commençant par
un double point.

.............................................
grep '^:' glawiSplit__1.xml > etymologies.txt
.............................................

4.2.3 Patron des étymologies
----------------------------

Le patron utilisé est : {{étyl}}.
Il est documenté sur cette page :
    https://fr.wiktionary.org/wiki/Mod%C3%A8le:%C3%A9tyl
    
Sa syntaxe :
    {{étyl|
        code-langue-1|
        code-langue-2|
        mot=mot1|
        tr=translittération1|
        type=code-gramm|
        num=lemme|
        sens=traduction1
    }}

Quelques exemples :
    o {{étyl|la|fr|mot=alternus}} pour alterne
    o {{étyl|la|nl|mot=albumen|sens=blanc d’œuf}} pour albumen

Le code-langue-1 indique la langue d'origine de l'étymon.
Le code-langue-2 indique la langue de l'entrée.
    On peut s'interroger sur le "nl" du deuxième exemple qui pour nous est une
    erreur, il devrait être mis "fr". Cela a été corrigée dans la version en
    ligne d'albumen (au 23 janvier 2019 à 16h29).

4.2.4 Les langues des étymons par fréquence décroissante
--------------------------------------------------------

Pour extraire toutes les utilisations du patron, nous utilisons les commandes
suivantes :

................................................................................
grep ".tyl|" glawiSplit__1.xml | sed -r 's/^[^\{]*\{\{[^t]+tyl\|([^\}]*)\}.*/\1/g' > 4_2_4_a_etymologies_patrons.txt
................................................................................

Nous utilisons [^t]+ pour capturer le "é" qui n'est pas bien reconnu si nous
mettons seulement "." par sed. Pourtant, GNU sed gère l'unicode mais peut être 
dans MSYS une étape ne le gère pas. Voici des exemples tirés du fichier :

|it|fr|mot=lira
|la|fr|mot=interrogatio
|la|fr|mot=procrastino|dif=procrastinare

Nous faisons ensuite une extraction du code-langue-1 avec trie et unicité pour
obtenir une liste des langues d'étymons par ordre décroissant de fréquence :

................................................................................
sed -r 's/^([^\|]*)\|.*/\1/' etymologies_patrons.txt | sort | uniq -c | sort -r > 4_2_4_b_langues_etymons.txt
................................................................................

Les 10 langues d'origine des étymons les plus fréquentes sont :
   o 9 577 la
   o 1 031 grc
   o 1 025 fr
   o   752 en
   o   471 it
   o   454 indo-européen commun
   o   396 de
   o   343 es
   o   340 fro
   o   230 eo

4.3 Extraction et conversion des gloses
---------------------------------------

4.3.1 Signalement des gloses
----------------------------

Nous avons étudié l'entrée "tour" car elle contient deux sens :
    o la tour, construction défensive élevée
    o le tour, déplacement qui finit là où il commence
    
Les deux gloses / sens sont traduits par deux sections :
    o === {{S|nom|fr|num=1}} ===
    o === {{S|nom|fr|num=2}} ===

Les gloses sont donc des sections de la forme : 

=== {{S|pos|code-langue|num=nombre}} ===

Le nombre est là pour distinguer entre plusieurs sens de même catégorie du
discours (part of speech).

4.3.2 Extraction et comptage de l'ensemble des gloses
-----------------------------------------------------

Selon la page : 
https://fr.wiktionary.org/wiki/Wiktionnaire:Liste_de_tous_les_mod%C3%A8les/Titres_de_sections/Liste_automatique

Il existe 67 catégories du discours différentes, certaines utilisant le même
code. Nous avons fait le fichier 4_3_2_a_pos.txt qui reprend ces catégories par
un copier-coller de la page. Pour que cela marche avec cut et uniq
avec Notepad++, nous convertissons les tabulations en espace et les fins de
ligne en UNIX.

Nous exécutons ensuite la commande suivante :

....................................................................
cut -d ' ' -f 1 4_3_2_a_pos.txt | sort | uniq > 4_3_2_b_pos_uniq.txt
....................................................................

Pour obtenir dans le fichier 4_3_2_b_pos_uniq.txt juste les codes uniques. 

Puis, par concaténation manuelle nous fabriquons la commande suivante :

................................................................................
grep -E "{{S\|adj|{{S\|adj-dém|{{S\|adj-excl|{{S\|adj-indéf|{{S\|adj-int|{{S\|adj-num|{{S\|adj-pos|{{S\|adj-rel|{{S\|adjectif|{{S\|adv|{{S\|adv-ind|{{S\|adv-int|{{S\|adv-pron|{{S\|adv-rel|{{S\|adverbe|{{S\|aff|{{S\|art|{{S\|art-déf|{{S\|art-indéf|{{S\|art-part|{{S\|article|{{S\|circon|{{S\|circonf|{{S\|class|{{S\|classif|{{S\|conj|{{S\|conj-coord|{{S\|conjonction|{{S\|copule|{{S\|dét|{{S\|encl|{{S\|faute|{{S\|gismu|{{S\|inf|{{S\|interf|{{S\|interj|{{S\|lettre|{{S\|loc|{{S\|loc-phr|{{S\|locution|{{S\|locution-phrase|{{S\|nom|{{S\|nom-fam|{{S\|nom-pr|{{S\|nom-sciences|{{S\|num|{{S\|numeral|{{S\|numér|{{S\|onom|{{S\|onoma|{{S\|part|{{S\|part-num|{{S\|particule|{{S\|patronyme|{{S\|phr|{{S\|phrase|{{S\|post|{{S\|postpos|{{S\|procl|{{S\|pronom|{{S\|pronom-adj|{{S\|pronom-dém|{{S\|pronom-indéf|{{S\|pronom-int|{{S\|pronom-per|{{S\|pronom-pers|{{S\|pronom-pos|{{S\|pronom-rel|{{S\|pronom-réfl|{{S\|prov|{{S\|pré-nom|{{S\|pré-verb|{{S\|préf|{{S\|prénom|{{S\|prép|{{S\|quantif|{{S\|rad|{{S\|radical|{{S\|rafsi|{{S\|sino|{{S\|sinog|{{S\|sinogramme|{{S\|substantif|{{S\|suf|{{S\|suff|{{S\|symb|{{S\|var-typo|{{S\|variante|{{S\|verb" glawiSplit__1.xml > gloses1.txt
................................................................................

Nous avons ainsi toutes les patrons indiquant une glose avec ses paramètres.

Pour faire un compte, il suffit d'effectuer :

................................................................................
cut -d '|' -f 2 gloses1.txt | sed -r 's/^([a-z]*).*/\1/' | sort | uniq -c | sort -r > 4_3_2_c_gloses_count.txt
................................................................................

Les 5 premières lignes du fichier gloses_count.txt sont :
    o 52 313 nom
    o 13 137 verbe
    o 12 050 adjectif
    o  3 608 adverbe
    o  2 857 variantes

4.3.3 Encodage des hyperliens
-----------------------------

Les hyperliens sont encodés en wikicode ainsi : [[ ]]

Ils peuvent eux aussi avec des arguments à l'intérieur, séparés par |

Les liens vers des fichiers ont "Fichier:" comme préfixe :

[[Fichier:Tour de Montlhéry.jpg|vignette|La '''tour''' de Montlhéry ''(sens 1)'']]

................................................................................
sed -r 's/\[\[(Fichier|Image):[^\]*\]\]//g' glawiSplit__1.xml | \
sed -r 's/\[\[[^:]+:([^]\|]*)\|[^]]*\]\]/\1/g' | \
sed -r 's/\[\[[^:]+:([^]]*)\]\]/\1/g' | \
sed -r 's/\[\[([^]\|]*)\|[^]]*\]\]/\1/g' | \
sed -r 's/\[\[([^]]*)\]\]/\1/g' > 4_3_3_nohyperlink.txt
................................................................................

Notes :
    o Le premier sed remplace les liens de la forme "[[Fichier:...]" ou ceux
      de la forme "[[Image:...]]" par rien.
    o Le deuxième sed remplace les liens de la forme [[langue:xxx|yyy]] par xxx.
    o Le troisième sed remplace les liens de la forme [[langue:xxx]] par xxx.
    o Le quatrième sed remplace les liens de la forme [[xxx|yyy]] par xxx.
    o Le cinquième sed remplace les liens de la forme [[xxx]] par xxx.

********************************************************************************
Discussion sur .*?
    o Théoriquement, en rendant l'opérateur * non greedy avec ?, cela devrait
      éviter de faire les deux derniers sed, mais sur notre environnement, cela
      ne marche pas. Après bien des interrogations, nous avons essayé l'exemple
      suivant donné dans le cours :
      ..............................................................
      echo "un lapin au vin blanc" | sed -r 's/l.*?n/DICTIONNAIRE/g'
      ..............................................................
      Malgré l'opérateur ? modificateur du * pour le rendre non greedy
      et le modificateur global g pour effectuer le remplacement autant de fois 
      que le motif est trouvé, nous obtenons : un DICTIONNAIREc au lieu de :
      un DICTIONNAIRE au vin bDICTIONNAIREc
    o Nous avons également testé avec le même résultat ici (GNU Sed 4.4) :
      https://www.tutorialspoint.com/execute_bash_online.php
      ainsi que sur la machine virtuelle fournie par l'UT2J.

Notre première solution se fonde sur le remplacement de .*?x par [^x]*x 

Note : après bien des questionnements, nous avons compris que sed ne VEUT pas
que le caractère ] soit échappé lorsqu'il est utilisé dans une classe qu'on ne
veut pas. Ainsi si x = ], on a à la base .*?\] et la transformation devrait donc
être : [^\]]*\] . Mais le ] dans l'anticlasse (commençant par ^) ne doit PAS
être échappé pour que cela marche. La bonne conversion est donc : [^]]*\]
********************************************************************************

Une autre solution serait d'utiliser Perl pour palier le problème de .*? :

................................................................................
perl -CSDA -p -e 's/\[\[(Fichier|Image):.*\]\]//' glawiSplit__1.xml | perl -CDSA  -p -e 's/\[\[[^\:]*:(.*)\]\]/\1/g' | perl -CSDA -p -e 's/\[\[([^#]*?)\#.*?\]\]/\1/g' | perl -CSDA -p -e 's/\[\[([^\|]*?)\|.*?\]\]/\1/g' | perl -CSDA -p -e 's/\[\[(.*?)\]\]/\1/g' | perl -CDSA -p -e 's/(\[\[)|(\]\])//g' > nohyperlink2.txt
................................................................................

4.3.4 Encodage du gras et de l'italique
---------------------------------------

Le gras est encodé par '''mot en gras'''.
L'italique est encodé par ''mot en italique''.

On fait l'opération suivante :

...................................................
sed -r "s/'''?//g" glawiSplit__1.xml > noquotes.txt
...................................................

Pour encoder le formatage au lieu de le supprimer :

................................................................................
sed -r "s/'''(.*?)'''/<b>\1<\/b>/g" glawiSplit__1.xml| sed -r "s/''(.*?)''/<i>\1<\/i>/g" > noquotes.txt
................................................................................

Notes : comme expliqué plus haut, utiliser le ? pour rendre le * non greedy ne
marche pas sur ma configuration. Il le faut car sinon :
''aaa'' ''bbb'' est transformé en <i>aaa'' ''bbb</i> car .* "mange" le plus
possible. Alors qu'avec .?*, il devrait s'arrêter au premier '' qu'il rencontre
ce qui est essentiel pour différencier un '' fermant d'un '' ouvrant pour 
remplacer par <i> ou <\i>.

Une solution est d'utiliser Perl avec :
................................................................................
perl -CSDA -p -e "s/'''(.*?)'''/<b>\1<\/b>/g" glawiSplit__1.xml | perl -CSDA -p -e "s/''(.*?)''/<i>\1<\/i>/g" > quotebold.txt
................................................................................

Mais nous avons un autre problème : la succession de deux balises wikicode comme
dans l'exemple :

#* '''''Pendant''' de baudrier ou de ceinturon.''

La bonne traduction est <i><b>Pendant</b> de baudrier ou de ceinturon.</i>
Alors que la nôtre est <b><i>Pendant</b> de baudrier ou de ceinturon.</i>
Ce qui ne respecte pas les règles XHTML (HTML est lui plus permissif) :
    o on ne doit pas fermer une balise parente puis fermer une balise fille.
Cela vient du fait que nous matchons en premier le triple. On corrige ainsi
notre commande :

................................................................................
perl -CSDA -p -e "s/'''([\w ].*?)'''/<b>\1<\/b>/g" glawiSplit__1.xml | perl -CSDA -p -e "s/''(.*?)''/<i>\1<\/i>/g" > quotebold.txt
................................................................................

Nous obtenons la bonne traduction. Nous précisons CSDA pour signaler à Perl de
fonctionner en Unicode pour que \w reconnaisse bien toutes les lettres non
romanes.

4.3.5 Les 10 patrons qui restent
--------------------------------

Nous cumulons les deux traitements précédents pour obtenir un fichier sans
liens et en ayant converti le gras et l'italique.

................................................................................
perl -CSDA -p -e 's/\[\[(Fichier|Image):.*\]\]//' glawiSplit__1.xml | perl -CDSA  -p -e 's/\[\[[^\:]*:(.*)\]\]/\1/g' | perl -CSDA -p -e 's/\[\[([^#]*?)\#.*?\]\]/\1/g' | perl -CSDA -p -e 's/\[\[([^\|]*?)\|.*?\]\]/\1/g' | perl -CSDA -p -e 's/\[\[(.*?)\]\]/\1/g' | perl -CDSA -p -e 's/(\[\[)|(\]\])//g' | perl -CSDA -p -e "s/'''([\w ].*?)'''/<b>\1<\/b>/g" | perl -CSDA -p -e "s/''(.*?)''/<i>\1<\/i>/g" > nothing.txt
................................................................................

Il faut à présent sélectionner les patrons en enlevant tous les patrons traitant
des types de mots déjà analysés :

................................................................................
grep -E "{{" nothing.txt | grep -v -E "{{S\|adj|{{S\|adj-dém|{{S\|adj-excl|{{S\|adj-indéf|{{S\|adj-int|{{S\|adj-num|{{S\|adj-pos|{{S\|adj-rel|{{S\|adjectif|{{S\|adv|{{S\|adv-ind|{{S\|adv-int|{{S\|adv-pron|{{S\|adv-rel|{{S\|adverbe|{{S\|aff|{{S\|art|{{S\|art-déf|{{S\|art-indéf|{{S\|art-part|{{S\|article|{{S\|circon|{{S\|circonf|{{S\|class|{{S\|classif|{{S\|conj|{{S\|conj-coord|{{S\|conjonction|{{S\|copule|{{S\|dét|{{S\|encl|{{S\|faute|{{S\|gismu|{{S\|inf|{{S\|interf|{{S\|interj|{{S\|lettre|{{S\|loc|{{S\|loc-phr|{{S\|locution|{{S\|locution-phrase|{{S\|nom|{{S\|nom-fam|{{S\|nom-pr|{{S\|nom-sciences|{{S\|num|{{S\|numeral|{{S\|numér|{{S\|onom|{{S\|onoma|{{S\|part|{{S\|part-num|{{S\|particule|{{S\|patronyme|{{S\|phr|{{S\|phrase|{{S\|post|{{S\|postpos|{{S\|procl|{{S\|pronom|{{S\|pronom-adj|{{S\|pronom-dém|{{S\|pronom-indéf|{{S\|pronom-int|{{S\|pronom-per|{{S\|pronom-pers|{{S\|pronom-pos|{{S\|pronom-rel|{{S\|pronom-réfl|{{S\|prov|{{S\|pré-nom|{{S\|pré-verb|{{S\|préf|{{S\|prénom|{{S\|prép|{{S\|quantif|{{S\|rad|{{S\|radical|{{S\|rafsi|{{S\|sino|{{S\|sinog|{{S\|sinogramme|{{S\|substantif|{{S\|suf|{{S\|suff|{{S\|symb|{{S\|var-typo|{{S\|variante|{{S\|verb" > nothing2.txt
................................................................................
 
A partir du fichier nothing2.txt obtenu, nous analysons les patrons restant :

................................................................................
perl -CSDA -p -e 's/(.*?)\{\{(.*?)\}\}(.*?)/\2/g' nothing2.txt > nothing3.txt

sort nothing3.txt | uniq -c | sort -r > nothing4.txt

head -n 10 nothing4.txt > last_patrons.txt
................................................................................

Voici les 10 patrons les plus fréquents restant :
    o 62 456 S|étymologie ===
    o 19 976 S|prononciation ===
    o 17 225 langue|fr ==
    o 16 420 trad-fin
    o 15 273 S|références ===
    o 14 115 S|synonymes ====
    o 12 881 S|traductions ====
    o 10 589 S|voir aussi ===
    o 10 386 S|dérivés ====
    o  9 129 trad-début
J'ai laissé les "=" finaux pour analyser le niveau de titre.

Nous avons observés ces dix patrons à l'aide de grep par exemple pour trois
d'entre eux :

..........................................................
grep -E "\{\{S\|.+tymologie" glawiSplit__1.xml | head -n 10
grep -E trad-fin avant.txt | head -n 10
grep -E trad-début avant.txt | head -n 10
..........................................................

À partir de ces observations, nous avons construit des commandes pour remplacer
les 10 patrons les plus fréquents pour rendre plus lisible le Wikicode.

Nous avons notamment choisi de traduire :
{{trad-début|Composés chimique en chimie (1)}}
    xxx
{{trad-fin}}
Par :
<p><u>Composés chimique en chimie (1)</u> :
    xxx
</p>

À chaque ligne suivante correspond un changement de patron :

................................................................................
perl -CSDA -p -e 's/=== \{\{S\|.tymologie\}\} ===/<h3>Etymologie<\/h3>/g' glawiSplit__1.xml | \
perl -CSDA -p -e 's/=== \{\{S\|prononciation\}\} ===/<h3>Prononciation<\/h3>/g' | \
perl -CSDA -p -e 's/=== \{\{S\|r.f.rences\}\} ===/<h3>References<\/h3>/g' | \
perl -CSDA -p -e 's/=== \{\{S\|synonymes\}\} ===/<h3>Synonymes<\/h3>/g' | \
perl -CSDA -p -e 's/=== \{\{S\|traductions\}\} ===/<h3>Traductions<\/h3>/g' | \
perl -CSDA -p -e 's/=== \{\{S\|voir aussi\}\} ===/<h3>Voir aussi<\/h3>/g' | \
perl -CSDA -p -e 's/=== \{\{S\|d.riv.s\}\} ===/<h3>Derives<\/h3>/g' | \
perl -CSDA -p -e 's/\{\{trad-d.but\|(.*?)\}\}/<p><u>\1<\/u> :/g' | \
perl -CSDA -p -e 's/\{\{trad-fin\}\}/<\/p>/g'> last10.txt
................................................................................

Nous avons utilisé le "." à chaque caractère accentué.

5. GLÀFF [GLÀFF]
================

5.1 Nombre de formes, nombre de lemmes
--------------------------------------

Nous décompressons le lexique à partir de l'archive GLÀFF-1.2.1.tar.bz.
Nous obtenons le fichier GLAFF-1.2.2.txt.

Pour mettre au point nos scripts, nous créons miniGLÀFF.txt avec la commande :

............................................
head -n 1000 GLAFF-1.2.2.txt > miniGLÀFF.txt
............................................

Exemple de ligne :
affluent|Vmip3p-|affluer|a.fly|a.fly|9|0.318|187|6.485|369|1.672|1207|5.498|500|0.393|1929|1.538

Pour compter le nombre de formes :

.....................
wc -l GLAFF-1.2.2.txt
.....................

On a 1 406 857 formes.

Pour compter le nombre de lemmes :

.....................................................
cut -d "|" -f 3 GLAFF-1.2.2.txt | sort | uniq | wc -l
.....................................................

On prend la colonne n°3 (en comptant à partir de un) qui contient le lemme, on
trie puis on applique uniq et on compte le nombre de ligne. On a 164 291 lemmes.

5.2 Trie des étiquettes morphosyntaxiques par fréquence décroissante
--------------------------------------------------------------------

L'étiquette est dans la deuxième colonne (en comptant à partir de un). On
effectue donc la commande suivante :

...........................................................................
cut -d "|" -f 2 GLAFF-1.2.2.txt | sort | uniq -c | sort -r > etiquettes.txt
...........................................................................

Les 10 étiquettes les plus fréquentes sont les 10 premières lignes du fichier.
Le format utilisé pour les étiquettes est GRACE. 
Sa documentation est accessible sur ce site :
http://redac.univ-tlse2.fr/GLÀFFoli/about/posGrace.jsp

Nous présentons ici les 10 étiquettes les plus fréquentes avec la signification
du code GRACE :
    o 54 226 Ncfs    Nom commun féminin singulier
    o 50 532 Ncms    Nom commun masculin singulier
    o 45 223 Ncfp    Nom commun féminin pluriel
    o 42 406 Ncmp    Nom commun masculin pluriel
    o 39 696 Afpfs   Adjectif qualificatif féminin singulier
    o 36 652 Afpfp   Ajdectif qualificatif féminin pluriel
    o 25 416 Afpms   Adjectif qualificatif masculin singulier
    o 23 646 Afpmp   Adjectif qualificatif masculin pluriel
    o 21 801 Vmip3s- Verbe sous forme flexionnelle à l'indicatif 3e personne
                     du singulier
    o 21 798 Vmcp3s- Verbe sous forme flexionnelle au conditionnel 3e personne
                     du singulier

5.3 Formes fléchis du verbe googler
-----------------------------------

Une étiquette GRACE commençant par Vmn est un verbe à l'infinitif.
Nous ne voulons que les formes fléchies du verbe googler.

Première étape, filtrer pour n'avoir que le lemme googler :

......................................................
grep -E "V......\|googler\|" GLAFF-1.2.2.txt > tmp.txt
......................................................

Seconde étape, filtrer pour n'avoir que les formes fléchies et pas l'infinitif :

...........................................
grep -v -E "\|Vmn----\|" tmp.txt > tmp2.txt
...........................................

Dernière étape, couper pour n'avoir que la première colonne :

............................................
cut -d "|" -f 1 tmp2.txt | sort > google.txt
............................................

Et on obtient toutes les formes fléchies du verbe googler sans son infinitif.
Il y en a 50 (avec wc -l).
En n'ayant que la forme, nous avons donc des doublons et pour les supprimer nous
pouvons faire :

...............
uniq google.txt
...............

Ce qui ramène à 38 formes et nous amène à réfléchir sur la notion de forme :

    Une forme fléchie peut être comprise simplement comme une suite de lettre :
        parle = parle
    Mais une forme fléchie peut être aussi comprise comme une forme ET une
    catégorisation. Ainsi dans cette optique :
        parle|Vmip1s- (indicatif, présent, 1ère personne, singulier)
        parle|Vmmp2s- (impératif, présent, 2ème personne, singulier)
    ne sont PAS la même forme, alors que dans la première optique, si.
    Nous préciserons à chaque fois dans quelle optique nous nous plaçons.

5.4 Formes fléchis avec étiquettes morphosyntaxiques du verbe hacker
--------------------------------------------------------------------

Dans la question précédente, nous avons compris la mention :
    formes fléchies (uniquement) 
comme le fait de ne pas prendre l'infinitif.

Pour les questions suivantes, la mention formes fléchies n'est pas suivie de
uniquement, nous pensons donc qu'il faut prendre également l'infinitif.

En effet, l'infinitif est une forme fléchie, avec le suffixe -er, qui a la
particularité d'avoir été choisie pour représenter le lemme, c'est la forme
canonique ou de citation du lemme.

Nous prenons donc dans les questions suivantes l'infintif également en 
considération, sachant que sa suppression peut se faire simplement par l'ajout
du filtre suivant :

........................
grep -v -E "\|Vmn----\|"
........................

Pour toutes les formes fléchies du verbe hacker, nous reprenons les étapes pour
googler en gardant l'infinitif et en changeant juste la dernière étape pour
garder également l'étiquette :

............................................................................
grep -E "Vm.....\|hacker\|" GLAFF-1.2.2.txt | cut -d "|" -f 1,2 > hacker.txt
............................................................................

Comme il nous est demandé la forme fléchie et l'étiquette, nous nous plaçons
donc dans cette optique. Le fichier résultat a 51 formes. Par rapport aux 50 de
googler dans la même optique, la différence est l'infinitif.

5.5 Nombre de formes fléchies pour chaque verbe
-----------------------------------------------

Première chose, nous allons prendre que les verbes, avec les infinitifs :

................................................
grep -E "\|V......\|" GLAFF-1.2.2.txt > tmp1.txt
................................................

Puis nous allons garder seulement le lemme car nous voulons simplement compter.
En effet, si on a :
    o dansons|...|danser
    o dansez|...|danser
On aura :
    o danser
    o danser
Donc on sait qu'on a deux formes fléchies pour danser mais on ne sait pas
lesquelles. Il suffit ensuite faire sort, uniq -c puis sort -r.

..................................................................
cut -d "|" -f 3 tmp1.txt | sort | uniq -c | sort -r > flechies.txt
..................................................................

Nous voulons ensuite savoir combien il y a de nombres de formes fléchies, en
considérant donc qu'une forme et à la fois une suite de lettres et une
catégorisation. Pour cela, nous faisons la commande suivante :

.............................................................................
sed -r 's/[^0-9]+([0-9]+).*/\1/' flechies.txt | uniq -c | sort -r > count.txt
.............................................................................

Les 13 premiers résultats sont :
    o 17 003 verbes ont 51 formes fléchies
    o  2 789 verbes ont 48 formes fléchies
    o    505 verbes ont 50 formes fléchies
    o    353 verbes ont 72 formes fléchies
    o    120 verbes ont 47 formes fléchies
    o    102 verbes ont  1 formes fléchies
    o     92 verbes ont 52 formes fléchies
    o     85 verbes ont 49 formes fléchies
    o     75 verbes ont 69 formes fléchies
    o     60 verbes ont 53 formes fléchies
    o     26 verbes ont 71 formes fléchies
    o     26 verbes ont 10 formes fléchies
    o     12 verbes ont 98 formes fléchies
    o     12 verbes ont 60 formes fléchies

Le minimum de formes fléchies est 1 pour 102 verbes.
Le maximum de formes fléchies est 119 pour 1 verbes (broebeler).
La moyenne est de 51 formes fléchies par verbe.
79% des verbes ont 51 formes fléchies. 
13% des verbes ont 48 formes fléchies.
En comptant les verbes de 51 et 48 formes, on obtient 92%.

Les verbes avec très peu de formes fléchis s'expliquent car le matériau de base
à partir duquel a été constitué GLÀFF peut ne pas contenir toutes les formes
fléchies des verbes. On a par exemple :
    fazéier, embaquer, dérompre, déradicaliser, décarpiller, débocarder,
    débadger, câlicer, cassecouiller, carapaçonner, bréquer, brasseïer
    bamboucher

Les verbes surabondants peuvent s'expliquer par une possibilité de choix dans
la racine qui permet ensuite d'avoir toutes les formes fléchies en double.
Nous avons étudié le verbe bunkeriser et avons vu que la plupart de ses formes
étaient doublées, sans que cela soit systématique, ainsi on a :
    o bunkériserions    Vmcp1p-
    o bunkeriserions    Vmcp1p-

5.6 Verbes surabondants et verbes défectifs
-------------------------------------------

Un verbe surabondant à plusieurs formes possibles pour une même personne, un
même genre, un même temps, un même mode.
Un verbe défectif à des formes manquantes.
En nous basant sur les résultats de la partie précédente, nous prenons 51 comme
nombre normal de formes fléchies pour un verbe, en considérant une forme comme
une suite de lettres et une catégorisation.

Il suffit de reprendre le fichier flechies.txt et de prendre toutes les lignes
différentes de 51. Avec un nombre supérieur pour les verbes surabondant, avec
un nombre inférieur pour les verbes défectifs. Ce qui donne pour les 
surabondants :

................................................................................
grep -E "(72)|(52)|(69)|(53)|(71)|(98)|(60)|(55)|(54)|(68)|(81)|(56)|(93)|(73)|(64)|(85)|(70)|(57)|(97)|(74)|(59)|(89)|(82)|(78)|(77)|(66)|(63)|(62)|(61)|(58)|(199)" flechies.txt > surabondants.txt
................................................................................

Et pour les défectifs on fait l'inverse en oubliant pas d'ajouter 51 :

................................................................................
grep -v -E "(51)|(72)|(52)|(69)|(53)|(71)|(98)|(60)|(55)|(54)|(68)|(81)|(56)|(93)|(73)|(64)|(85)|(70)|(57)|(97)|(74)|(59)|(89)|(82)|(78)|(77)|(66)|(63)|(62)|(61)|(58)|(199)" flechies.txt > defectifs.txt
................................................................................

On a :
    o wc -l surabondants.txt =>          699
    o wc -l defectifs.txt    =>        3 708
    o verbes avec 51 formes fléchies  17 003
    ----------------------------------------
    o wc -l flechies.txt     =>       21 410
Le compte est bon.

6. Comparaison des nomenclatures de GLÀFF et de Morphalou [MORPH]
=================================================================

Pour les deux points suivants, nous construisons un fichier de toutes les
formes du Morphalou. Nous avons repéré qu'elles sont dans :
<orthography>mousse</orthography>

................................................................................
grep -E "<orthography>" Morphalou-2.0.xml | sed -r 's/^[^<]*<orthography>([^<]*)<\/orthography>.*$/\1/' | sort | uniq > morphalou.txt
................................................................................

wc -l nous informe qu'il comporte 410 441 formes.

Nous construisons un fichier des formes, en ne considérant que la suite de
lettres cette fois-ci, à partir du GLÀFF :

.........................................................
cut -d "|" -f 1 glaff-1.2.2.txt | sort | uniq > glaff.txt
.........................................................

wc -l nous informe qu'il comporte 1 082 688 formes.

Nous faisons ensuite un diff des deux :

.......................................
diff morphalou.txt glaff.txt > diff.txt
.......................................

Nous avons à présent un fichier à partir duquel travailler.

6.1 Liste des formes dans le Morphalou et pas dans GLÀFF
--------------------------------------------------------

Les lignes du fichier diff.txt commençant par < indique une forme présente dans
le Morphalou et pas dans GLÀFF :

..........................................
grep -E "^<" diff.txt > pas_dans_glaff.txt
..........................................

wc -l nous informe qu'il y en 58 168.

6.2 Liste des formes dans GLÀFF et pas dans Morphalou
-----------------------------------------------------

Les lignes du fichier diff.txt commençant par > indique une forme présente dans
le GLÀFF et pas dans Morphalou :

..............................................
grep -E "^>" diff.txt > pas_dans_morphalou.txt
..............................................

wc -l nous informe qu'il y en 730 415.

+----------------------------------------------+------------+-----------+
|                                              |   GLÀFF    | Morphalou |
+----------------------------------------------+------------+-----------+
| Nombre de forme (suite de lettres seulement) | 1 082 688  |  410 441  |
+----------------------------------------------+------------+-----------+
| Présente uniquement dans la ressource        |   730 415  |   58 168  |
+----------------------------------------------+------------+-----------+
| Pourcentage de non recouvrement              |        67% |       14% |
+----------------------------------------------+------------+-----------+

On constate donc que le GLÀFF est beaucoup plus riche que le Morphalou, mais
qu'il gagnerait à être enrichi des 58 168 formes que le Morphalou possède et pas
lui.

7. Conversion de GLÀFF en format CSV [CSV]
==========================================

7.1 Conversion du GLÀFF en CSV
------------------------------

Nous devons produire une version de GLÀFF qui contienne :
    o forme
    o étiquette
    o lemme
    o transcriptions API (séparées par un ;)

Nous sélectionnons les champs demandés avec la commande suivante :

.............................................
cut -d "|" f 1,2,3,4 miniglawi.txt > tmp1.txt
.............................................

Puis nous remplaçons le | par un , :

......................................
sed -r 's/\|/,/g' tmp1.txt > final.csv
......................................

Et nous pouvons ensuite ouvrir final.csv avec OpenOffice, LibreOffice ou Excel.

================================================================================
Partie B : utilisation de Java [JAVA]
================================================================================

1. Environnement de travail [ENV]
---------------------------------

Pour cette partie, nous développons sur Windows avec un JDK et Notepad++.

Nous utilisons quotidiennement Eclipse à notre travail mais préférons un
environnement plus léger quand c'est possible.

Nous utilisons le script bat suivant pour compiler :

................................................................
@echo off
set JAVADIR=C:\tools\jdk-11.0.2\bin
echo ------------------ CLEANING ----------------------
if exist .\saxo\DGXHandler.class (
    del .\saxo\DGXHandler.class
)
if exist .\saxo\Inspector.class (
    del .\saxo\Inspector.class
)
echo ------------------ END CLEANING ------------------
echo ------------------ COMPILING ---------------------
%JAVADIR%\javac.exe -encoding utf8 .\saxo\DGXHandler.java -Xlint
echo ------------------ END COMPILING -----------------
if exist .\saxo\DGXHandler.class (
    if exist .\saxo\Inspector.class (
        echo ------------------ EXECUTING ---------------------
        %JAVADIR%\java.exe saxo.DGXHandler
        echo ------------------ END EXECUTING -----------------
    )
    if not exist .\saxo\Inspector.class (
        echo SOMETHING WENT WRONT WHILE COMPILING Inspector.java
    )
)
if not exist .\saxo\DGXHandler.class (
    echo SOMETHING WENT WRONT WHILE COMPILING
)
................................................................


2. Extractions à partir du GLAWI [EXT]
--------------------------------------

Nous avons créé une classe DGXHandler dans le package saxo qui étend la classe
DefaultHandler. Nous redéfinissons plusieurs méthodes :
    o La méthode startElement est appelée à la balise ouvrante. On a accès à ses
      attributs.
    o La méthode characters est appelée entre la balise ouvrante et la balise
      fermante.
    o La méthode endElement est appelée à la balise fermante.

2.1 Extraction de la nomenclature
---------------------------------

Nous parsons en premier un extrait de GLAWI pour extraite sa nomenclature.

Nous repérons les titres en détectant la balise ouvrante <titre>.
Nous stockons ensuite le titre dans la variable d'instance protected 
nomenclature de type ArrayList<String>. La méthode getTitles() permet d'y
accéder ensuite.

Nous l'affichons enfin sur la console et dans le fichier Nomenclature.txt.
Nous reproduisons ici ses 20 premières lignes :
    o -athlon
    o -iser
    o -ité
    o -ière
    o -lâtre
    o -oir
    o -oyer
    o -phyte
    o -thèque
    o 22 mètres
    o 89
    o Abbevilloise
    o Abbevilloises
    o Abitibien
    o Adolphe
    o Amandine
    o Amélie
    o Astrolabe
    o Astérix
    o Belle

2.2 Table de fréquences des parties du discours
-----------------------------------------------

Nous repérons les catégories en détectant la balise ouvrante <pos> et en 
extrayant la valeur de son attribut type.

Nous stockons ensuite dans la variable d'instance protected pos de type
HashMap<String, Integer> les différentes parties du discours en comptant le
nombre de leurs occurrences. La méthode getPos() permet d'y accéder ensuite.

Nous l'affichons enfin sur la console et dans le fichier 
Table of frequencies.txt en triant par le nombre d'occurrences en descendant.

Le résultat est dans le fichier "2_02_Table of frequencies.txt" et reproduit
ici :

Key =                          verbe | Value =                          19754
Key =                            nom | Value =                           3076
Key =                       adjectif | Value =                           1397
Key =                        adverbe | Value =                             70
Key =                     nom propre | Value =                             47
Key =                        préfixe | Value =                             24
Key =                   interjection | Value =                             17
Key =                         prénom | Value =                             12
Key =                        suffixe | Value =                              9
Key =                      synonymes | Value =                              3
Key =         variante typographique | Value =                              2
Key =                     onomatopée | Value =                              2
Key =                         phrase | Value =                              2
Key =                 nom de famille | Value =                              2
Key =                        dérivés | Value =                              1
Key =                    traductions | Value =                              1
Key =                    préposition | Value =                              1
Key =                    conjonction | Value =                              1
Key =                   postposition | Value =                              1
Key =                pronom indéfini | Value =                              1

2.3 Types de marques lexicographiques
-------------------------------------

Nous repérons les marques lexicographiques en détectant la balise ouvrante
<label> puis nous récupérons la valeur son attribut type.
Nous la stockons dans une variable d'instance protected labelTypes de type
HashMap<String, Integer> en comptant le nombre d'occurrences. La méthode
getLabelTypes() permet d'y accéder ensuite.

Le résultat est dans le fichier "2_03_Table_of_label_types.txt" et reproduit
ici :

Key =                         domain | Value =                           1539
Key =                     diachronic | Value =                            496
Key =                    attitudinal | Value =                            306
Key =                            sem | Value =                            247
Key =                          other | Value =                            161
Key =                       diatopic | Value =                            109
Key =                 diafrequential | Value =                            102
Key =                           gram | Value =                             24
Key =                           loan | Value =                             13

En bonus, nous avons également étudié toutes les valeurs du type sem. Le
résultat est dans le fichier "2_03b_Table_values_of_sem_types.txt" et reproduit
ici :

Key =                         figuré | Value =                            104
Key =                  par extension | Value =                             87
Key =                 en particulier | Value =                             23
Key =                      métonymie | Value =                             11
Key =                   spécialement | Value =                              9
Key =                        ellipse | Value =                              4
Key =                         propre | Value =                              3
Key =                     absolument | Value =                              2
Key =                       analogie | Value =                              1
Key =                        apocope | Value =                              1
Key =                      métaphore | Value =                              1
Key =                   généralement | Value =                              1

2.4 Les 5 marques de domaine les plus fréquents
-----------------------------------------------

Nous regardons la balise <label>, son attribut type doit avoir la valeur domain.
On stocke et on compte dans la variable protégée domains de type
HashMap<String, Integer>. On y accède ensuite avec la méthode getDomains().

Le résultat est dans le fichier "2_04_Five most frequent domains.txt", seul les
5 plus fréquents sont gardés. Nous avons considéré qu'il peut y avoir des
ex-aequo. Le contenu du fichier est reproduit ici :

1]===================
Key =                      botanique | Value =                            151
2]===================
Key =                         chimie | Value =                            131
3]===================
Key =                       médecine | Value =                             91
4]===================
Key =                       zoologie | Value =                             72
5]===================
Key =                       histoire | Value =                             50

2.5a Entrées avec au moins une définition dans le domaine le plus fréquent
--------------------------------------------------------------------------

Le domaine le plus fréquent est "botanique".

Il faut donc repérer toutes les entrées avec au moins une définition avec :
<label type="domain" value="botanique"/>. On a déjà l'entrée stockée dans la
variable nomenclature, il suffit alors de la stocker dans une autre variable
protégée, entreeAtLeastOneBotanique, de type ArrayList<String>.

On compte 151 résultats, nous en reproduisons ici les 10 premiers :
    o actée
    o albédo
    o aléné
    o aoutement
    o aouter
    o aouter
    o aoûtement
    o apiculé
    o appendiculé
    o aranéeux

Les autres sont dans le fichier 
"2_05a_At_least_one_definition_in_botanique.txt".

2.5b Entrées avec toutes les définitions dans le domaine le plus fréquent
-------------------------------------------------------------------------

Le domaine le plus fréquent est "botanique".
Cette fois-ci il faut que TOUTES les définitions d'une section pos soient
étiquetées par le domaine botanique pour que l'entrée figure dans notre lexique.

Notre algorithme est le suivant :
    o Quand on rencontre une balise ouvrante pos, on met les variables defCpt et
      botCpt à zéro.
    o On incrémente la variable defCpt à chaque balise ouvrante definition
    o On incrémente la variable botCpt à chaque balise label avec comme type
      domain et valeur botanique.
    o Quand on rencontre une balise fermante pos, on teste si defCpt == botCpt.
      Si c'est vrai, on stocke l'entrée dans la variable protégée
      entreeAllBotanique de type ArrayList<String>

On compte 129 résultats, nous en reproduisons ici les 10 premiers :
    o actée
    o aléné
    o aoutement
    o aoûtement
    o apiculé
    o arille
    o arillé
    o aristé
    o aréolé
    o badasse

Les autres sont dans le fichier 2_05b_All_definitions_in_botanique.txt.

Nous pouvons comparer les deux fichiers de résultats avec diff ou WinMerge.

2.6 Extraction des entrées avec leurs gloses qui sont des néologies
-------------------------------------------------------------------

Pour les trouver, il faut repérer <label type="diachronic" value="néologisme"/>.

En sortie, nous allons produire un fichier CSV et prendre :
    o article/titre
    o article/text/pos@type
    o article/text/pos/definitions/definition/txt
Pour chaque définition ayant l'étiquette recherchée.

Il faut bien faire attention de compiler le code avec l'option -encoding utf8 à
javac pour que le equals("néologisme") marche.

Nous utilisons comme séparateur le triplet "###" car des virgules et des points-
virgules sont présents dans le texte des définitions. Nous n'avons pas trouvé
le triplet "###" dans notre extrait, nous faisons l'hypothèse que c'est le cas
pour l'ensemble de Glawi.

De même, nous remplaçons dans le texte les nouvelles lignes par " // " pour ne
pas corrompre la structure de notre fichier.

Nous stockons les résultats dans la variable d'instance protégée neologisms de 
type ArrayList<String>.

Nous comptons 34 résultats, nous en reproduisons ici les 10 premiers :
    o animathèque###nom###Établissement qui conserve des films d'animation.
    o apprenant###nom###Personne qui apprend, en étant acteur de son apprentissage.
    o archéographie###nom###Archéographie : tel est le nom que Marc-Alain Ouaknin propose pour désigner cette recherche qui raccorde notre alphabet à l'alphabet protosinaïtique et la distinguer ainsi d'une recherche étymologique ou purement linguistique. Elle s'en démarque en effet par son caractère largement anthropologique:.. (Commentaire sur « Les Mystères de l'Alphabet » de Marc-Alain Ouaknin (Editions Assouline), par Yvette Reynaud-Kherlakian)
    o aroniste###nom###Tocquevilliens et aronistes de tout poil, l'histoire vous donnera forcément raison contre la racaille populaire que vous méprisez, et même le fait que sept Français sur dix désapprouvent le maintien du CPE est forcément une preuve supplémentaire du bien-fondé de la réforme.
    o auteurisant###adjectif###Ce film est un catalogue de tout ce qu'il y a de plus nul dans le cinéma français auteurisant qui se regarde filmer et oublie juste que les fauteuils de cinéma ne sont pas là pour faire jolis.
    o bombiner###verbe###A, noir corset velu des mouches éclatantes // Qui bombinent autour des puanteurs cruelles (Arthur Rimbaud, Voyelles)
    o capillotracteur###adjectif###Personne racontant des histoires difficiles à croire.
    o conspirationnisme###nom###Au contraire, elle pêche volontairement dans un conspirationnisme nihiliste qui la rend incapacitante dans le domaine des idées et de l'action métapolitique.
    o coupleux###adjectif###Cette moto toute simple, pourvue d'un bon châssis, d'un moteur coupleux et d'une esthétique exceptionnelle était plébiscitée depuis longtemps par les concessionnaires.

Les autres sont stockés dans le fichier "2_06_Neologismes.txt".

2.7 Extraction des entrées avec plusieurs gloses dont une seule est une néologie
--------------------------------------------------------------------------------

Nous créons une variable d'instance protégée de type ArrayList<String> nommée neologismsOnlyOne.

Nous reprenons le même format de sortie que pour la question précédente mais cette fois-ci, il ne
faut prendre que les entrées :
    o avec plusieurs gloses
    o avec une seule néologie

Nous appuyons sur la documentation de GLAWI qui stipule : "A definition contains a gloss" repérée à
http://redac.univ-tlse2.fr/lexiques/glawi/doc/definitions.html
Il y a donc un lien 1-1 entre une définition et une glose.

Néanmoins, une entrée, que nous assimilons à un <article> possède potentiellement plusieurs <pos>
qui possèdent potentiellement plusieurs <definitions>. Nous devons prendre les entrées qui ont 
plusieurs <definitions> avec une seule portant la marque de néologie mais qu'en est-il du nombre de
<pos> ? Une entrée avec 1 <pos> ayant 2 <definition> dont 1 seule étant une néologie passe, mais
qu'en est-il d'une entrée avec 2 <pos> ? Tous les <pos> doivent-ils valider la contrainte pour que
l'entrée soit sélectionnée ? Nous choisissons de résoudre cette inconnue ainsi :
    o Nous sélectionnons les pos ayant plus d'une définition dont 1 seule est une néologie
      Notre format de sortie sera donc de la forme :
          o title###pos###nb pos###nb définitions###dont néologie
      L'avant-dernière colonne doit toujours être supérieure à un et la dernière égale à un.
      
Nous obtenons 5 résultats reproduit ici en améliorant manuellement le rendu :
    +=================+==========+========+========+==========+
    | Titre           | Pos      | nb pos | nb def | dont néo |
    +=================+==========+========+========+==========+
    | archéographie   | nom      |   1    |   2    |    1     |
    +-----------------+----------+--------+--------+----------+
    | déclassifier    | verbe    |   1    |   3    |    1     |
    +-----------------+----------+--------+--------+----------+
    | googler         | verbe    |   1    |   2    |    1     |
    +-----------------+----------+--------+--------+----------+
    | néviplanchiste  | nom      |   1    |   2    |    1     |
    +-----------------+----------+--------+--------+----------+
    | objectal        | adjectif |   1    |   3    |    1     |
    +-----------------+----------+--------+--------+----------+

Ils sont stockés dans le fichier 
"2_07_Neologismes_plusieurs_def_only_one_neo.txt".

Chaque résultat ne comporte qu'un seul pos, notre interrogation trouve donc sa 
réponse dans les faits : le cas d'un article avec plusieurs pos ne se produit
pas.

2.8 Extraction des entrées importées du Littré
----------------------------------------------

Pour cette question, nous devons détecté la balise ouvrante <import>.
Une fois détectée, nous regardons son contenu. S'il est égal à "Littré", à la 
balise fermante </import> nous stockons l'entrée dans la variable d'instance
entreeFromLittre de type ArrayList<String>.

Le fichier de sortie, "2_08_From_Littre.txt", compte 1 893 résultats.

2.9 Extraction des entrées importées du Littré portant une marque de néologie
-----------------------------------------------------------------------------

Pour cette question, nous devons croiser deux listes :
    o neologisms
    o entreeFromLittre

Comme les éléments de neologisms sont des concaténations d'une entrée de
certaines de ses caractéristiques, nous utilisons split("###") pour retrouver 
l'entrée, vérifier si elle est dans entreeFromLittre. Si oui, nous la stockons
dans une variable de type ArrayList<String> nommée entreeFromLittreNeo.

Le fichier de sortie, "2_09_Neologism_from_Littre.txt",  compte 16 entrées que
nous reproduisons ci-dessous :
    o crépusculin
    o divagateur
    o divagueur
    o déclassifier
    o délité
    o emmascaradé
    o endimanchement
    o mariniste
    o mariniste
    o mésestimable
    o paysanesque
    o pensivité
    o prétendance
    o sylvanesque
    o transcendantisme
    o épigrammatiser

À la lecture de ces résultats, on peut intuiter que certains néologismes n'ont pas survécu au
temps, d'autres sont rentrés dans le vocabulaire courant comme "délité". La comparaison avec un
dictionnaire plus récent permettrait de connaître leurs destinées.

2.10 Extraction des entrées importées du Littré dont toutes les gloses sont des néologies
-----------------------------------------------------------------------------------------

Nous retrouvons dans cette question le problème soulevé pour la question 2.7 : 
que faire des articles à plusieurs <pos> ? Tous les <pos> doivent-ils valider la
contrainte pour que l'entrée y figure ?

Nous allons dans un premier temps produire dans la variable d'instance protégée
neologismsAll de type ArrayList<String> qui va contenir le format suivant :
    o title###pos###nb pos###nb définitions###dont néologie
      Les deux dernières colonnes seront toujours égales.
      Notre interrogation porte sur la valeur de la deuxième colonne.

Nous avons pour cette première étape 28 résultats reproduit ici avec un rendu
amélioré manuellement:
    +===================+==========+========+========+==========+
    | Titre             | Pos      | nb pos | nb def | dont néo |
    +===================+==========+========+========+==========+
    | animathèque       | nom      |    1   |    1   |     1    |
    | apprenant         | nom      |    1   |    1   |     1    |
    | aroniste          | nom      |    2   |    1   |     1    |
    | auteurisant       | adjectif |    1   |    1   |     1    |
    | bombiner          | verbe    |    1   |    1   |     1    |
    | capillotracteur   | adjectif |    1   |    1   |     1    |
    | conspirationnisme | nom      |    1   |    1   |     1    |
    | coupleux          | adjectif |    1   |    1   |     1    |
    | crépusculin       | adjectif |    1   |    1   |     1    |
    | divagateur        | adjectif |    1   |    1   |     1    |
    | divagueur         | nom      |    1   |    1   |     1    |
    | délité            | adjectif |    1   |    1   |     1    |
    | emmascaradé       | adjectif |    1   |    1   |     1    |
    | endimanchement    | nom      |    1   |    1   |     1    |
    | lacérable         | adjectif |    1   |    1   |     1    |
    | mariniste         | adjectif |    1   |    1   |     1    |
    | mariniste         | nom      |    3   |    1   |     1    |
    | mésestimable      | adjectif |    1   |    1   |     1    |
    | niviplanchiste    | nom      |    1   |    1   |     1    |
    | paysanesque       | adjectif |    1   |    1   |     1    |
    | pensivité         | nom      |    1   |    1   |     1    |
    | priorisation      | nom      |    1   |    1   |     1    |
    | prétendance       | nom      |    1   |    1   |     1    |
    | sylvanesque       | adjectif |    1   |    1   |     1    |
    | transcendantisme  | nom      |    1   |    1   |     1    |
    | tétrapilectomie   | nom      |    1   |    1   |     1    |
    | uqutéran          | nom      |    1   |    1   |     1    |
    | épigrammatiser    | verbe    |    1   |    1   |     1    |
    +-------------------+----------+--------+--------+----------+
    
Ces résultats sont issus du fichier "2_10a_Entry_with_pos_all_neo.txt".

Si l'on prend le cas de mariniste dont deux <pos> valident notre condition, on a en supprimant les
balises qui ne nous intéressent pas :
    <article>
        <title>mariniste</title>
        <meta>
            <import>Littré</import>
        </meta>
    <text>
      <pos type="adjectif" lemma="1" locution="0" gender="e" number="s">
        <definitions>
          <definition>
            <gloss>
              <labels>
                <label type="diachronic" value="néologisme"/>
              </labels>
              <txt>Relatif à Marine Le Pen et à sa politique.</txt>
            </gloss>
          </definition>
        </definitions>
      </pos>
      <pos type="nom" lemma="1" locution="0" homoNb="1" gender="e" number="s">
        <definitions>
          <definition>
            <gloss>
              <txt>Peintre de marine.</txt>
            </gloss>
          </definition>
        </definitions>
      </pos>
      <pos type="nom" lemma="1" locution="0" homoNb="2" gender="e" number="s">
        <definitions>
          <definition>
            <gloss>
              <labels>
                <label type="diachronic" value="néologisme"/>
              </labels>
              <txt>Partisan de Marine Le Pen.</txt>
            </gloss>
          </definition>
        </definitions>
      </pos>
    </text>
  </article>
Soit 3 <pos> :
    o adjectif
    o nom
    o nom
Chacun a une seule définition et seulement pour deux, le 1er et le troisième, 
elle est étiquetée comme néologisme. Devons-nous prendre mariniste alors,
sachant qu'une de ses <pos> a une définition sans marque de néologie mais que
les deux autres <pos> remplissent la condition :
    "toutes les gloses d'une définition portent une marque de néologie" ?
Nous choisissons de la prendre.

Nous pouvons donc ensuite croiser la liste entreeFromLittre avec neologismsAll.
Pour cela nous parcourons neologismsAll en splittant sur "###" et on vérifie si
l'entrée est dans la liste d'entreeFromLittre et on peuple une variable nommée
entreeFromLittreAllNeo.

Nous obtenons 14 résultats (en supprimant le doublon mariniste), reproduit ici :
    o crépusculin
    o divagateur
    o divagueur
    o délité
    o emmascaradé
    o endimanchement
    o mariniste
    o mésestimable
    o paysanesque
    o pensivité
    o prétendance
    o sylvanesque
    o transcendantisme
    o épigrammatiser

Ces résultats sont issus du fichier
"2_10b_Entry_from_Littre_with_pos_all_neo.txt".

3. Inspecteur de structure [INS]
--------------------------------

Pour cette dernière question, nous avons fait une autre classe, Inspector, dans
le même package.

Notre code se base sur une liste, level, qui va agir comme une pile où on met
les niveaux.

On convertit cette liste en une chaîne unique, une clé qui va permettre de
stocker le niveau dans une HashMap<String, Integer> pour compter les
occurrences.

Dans la clé, les niveaux sont séparés par des "/", les attributs par "/@".
Le caractère @ est inférieur à toutes les lettres, les attributs seront donc
affichés avant les balises filles.

Elle parcourt l'extrait et écrit dans un fichier :
    3.1 Les balises en les comptant par ordre alphabétique des niveaux, 
        sans attributs => fichier "3_01_alpha no attr.txt"
    3.2 Les balises en les comptant par ordre de fréquences décroissantes, 
        sans attributs => fichier "3_02_freq no attr.txt"
    3.3 Les balises en les comptant par ordre alphabétique des niveaux, 
        avec attributs => fichier "3_03_alpha attr.txt"
    3.4 Bonus : les balises en les comptant par ordre de rencontre, sans 
        attributs => fichier "3_04_encounter no attr.txt"

--------------------------------------------------------------------------------

                                    [FIN]
