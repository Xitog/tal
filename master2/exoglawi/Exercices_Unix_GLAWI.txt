
                         _____  _                   _ 
                        |  __ \| |                 (_)
                        | |  \/| |  __ _ __      __ _ 
                        | | __ | | / _` |\ \ /\ / /| |
                        | |_\ \| || (_| | \ V  V / | |
                         \____/|_| \__,_|  \_/\_/  |_|
                              
                              
                                                        Auteur : Damien Gouteux
                                                        Version : 2019-01-22-1a
                                                        
--------------------------------------------------------------------------------
Plan
--------------------------------------------------------------------------------

Table des matières [TAB]
    o Introduction [INTRO]
    o Partie A : utilisation des outils Unix [UNIX]
        o 1. Observation de fichiers volumineux [OBS]
            o 1.1 Décompression de l'extrait de dump du Wiktionnaire
            o 1.2 Observation du fichier décompressé
            o 1.3 Organisation de l'information
        o 2. Manipulations [MAN]
            o 2.1 Extraire la nomenclature
            o 2.2 Extraire la nomenclature en supprimant les pages méta
        o 3. Dump et pages du Wiktionnaire [DUM]
            o 3.1 Extraction d'un article
            o 3.2 Observation de la page en ligne
            o 3.3 Différences dans le wikicode
        4. Analyse et conversion du wikicode [ANA]
            o 4.1 Sections, sous-sections et patrons
                o 4.1.1 Extraction et trie des types de sections
                o 4.1.2 Comptes dans l'extrait
                o 4.1.3 Patrons les plus fréquents
            o 4.2 Langue des étymons
                o 4.2.1 Section étymologie
                o 4.2.2 Extraction des étymologies
                o 4.2.3 Patron des étymologies
                o 4.2.4 Les langues des étymons par fréquence décroissante
            o 4.3 Extraction et conversion des gloses
                o 4.3.1 Signalement des gloses
                o 4.3.2 Extraction et comptage de l'ensemble des gloses
                o 4.3.3 Encodage des hyperliens
                o 4.3.5 Les 10 patrons qui restent
        5. GLÀFF [GLÀFF]
            o 5.1 Nombre de formes, nombre de lemmes
            o 5.2 Trie des étiquettes morphosyntaxiques par fréquence
                  décroissante
            o 5.3 Formes fléchis du verbe googler
            o 5.4 Formes fléchis avec étiquettes morphosyntaxiques du verbe
                  hacker
            o 5.5 Nombre de formes fléchies pour chaque verbe 
            o 5.6 Verbes surabondants et verbes défectifs
        6. Comparaison des nomenclatures de GLÀFF et de Morphalou [MORPH]
            o 6.1 Liste des formes dans le Morphalou et pas dans GLÀFF
            o 6.2 Liste des formes dans GLÀFF et pas dans Morphalou
        7. Conversion de GLÀFF en format CSV [CSV]
            o 7.1 Conversion du GLÀFF en CSV
    o Partie B : utilisation de Java [JAVA]
        1. Environnement de travail

Rechercher par le code rapide pour directement aller à la section voulue.

================================================================================
Introduction [INTRO]
================================================================================

Ce document présente le travail fait pour les deux présentations de F. Sajous
sur Glawi dans l'UE Thématiques actuelles de la recherche en traitement 
automatique des langues.

La première partie traite des commandes Unix et la seconde de Java.

================================================================================
Partie A : utilisation des outils Unix [UNIX]
================================================================================

Pour cette partie, notre travail a été effectué avec MinGW + MSYS sur Windows.

Nous avons utilisé la partie glawiSplit__1.xml comme base de travail.

Nous avons parfois choisi de décomposer les commandes complexes avec des 
fichiers intermédiaires. Ces fichiers intermédiaires étaient autant de "point de
debug" dans notre mise au point.

Tous les fichiers sont reproductibles en suivant les commandes décrites ici.
Nous avons également mis dans le dossier fichiers_5_GLAFF quelques fichiers émis
par nos commandes pour la partie 5.

1. Observation de fichiers volumineux [OBS]
===========================================

1.1 Décompression de l'extrait de dump du Wiktionnaire
------------------------------------------------------

Nous travaillons sur le premier fichier d'extrait du dump du Wiktionnaire.
Nous le décompressons avec :

..............................
bzip2 -d glawiSplit__1.xml.bz2
..............................

1.2 Observation du fichier décompressé
--------------------------------------

Nous l'observons avec :

......................
less glawiSplit__1.xml
......................

Nous pouvous aussi prendre les 100 premières lignes et les mettre dans un autre
fichier :

......................................................
head -n 100 glawiSplit__1.xml > head_glawiSplit__1.xml
......................................................

1.3 Organisation de l'information
---------------------------------

La balise <page> indique une page qui possède un <titre> et une <revision>.
La révision contient le texte de la page à un moment donné dans la balise 
<text>. Dans les fichiers donnés, il n'y a qu'une révision par page.

Les informations sur le mot sont en WikiCode à l'intérieur de la balise <text> :
    o == {{langue|fr}} == Langue du mot
    o === {{S|nom|fr}} === Catégorie du discours et langue
    o ==== {{S|synonymes}} ==== Synonymes du mot
    o ==== {{S|traductions}} ==== Traductions du mot

2. Manipulations [MAN]
======================

2.1 Extraire la nomenclature
----------------------------

Nous cherchons la balise <title> qui indique une entrée :

............................................
grep \<title\> glawiSplit__1.xml > grep1.txt
............................................

Avec cut :

..............................................................
cut -d ">" -f 2 grep1.txt | cut -d "<" -f 1 > nomenclature.txt
..............................................................

Avec sed :

..........................................................................
sed 's/ *<title>//' grep1.txt | sed 's/<\/title>//' > nomenclature_bis.txt
..........................................................................

On obtient avec les deux méthodes le même fichier (vérifié avec WinMerge).

2.2 Extraire la nomenclature en supprimant les pages méta
---------------------------------------------------------

Les pages méta contiennent un ":" dans le titre.

Nous utilisons un grep inversé pour ne PAS sélectionner les lignes 
qui correspondent au motif :

...............................................................
grep -v " *: *" nomenclature_bis.txt > nomenclature_filtree.txt
...............................................................

La taille de cette nomenclature filtrée est obtenue avec :

..............................
wc -l nomenclature_filtree.txt
..............................

Elle compte 46 337 éléments.

3. Dump et pages du Wiktionnaire [DUM]
======================================

3.1 Extraction d'un article
---------------------------

Pour extraire le contenu de l'article "avant", nous le cherchons avec grep et
nous mettons les 1000 lignes avant et après dans un fichier intermédiaire :

...........................................................................
grep -A 1000 -B 1000 \<title\>avant\</avant\> glawiSplit__1.xml > avant.txt
...........................................................................

À la ligne 1001 du fichier en sortie nous avons bien "    <title>avant</title>".

3.2 Observation de la page en ligne
-----------------------------------

La page en ligne correspondant à ce dump est accessible à cette adresse :
https://fr.wiktionary.org/wiki/avant

Comme le Wiktionnaire change constamment, il n'est pas garanti que le dump que
nous avons corresponde exactement à ce qui est actuellement sur la page, 
surtout s'il est ancien.

3.3 Différences dans le wikicode
--------------------------------

Nous avons copié le wikicode du dump qui se trouve dans la balise <text> et 
l'avons comparé au wikicode actuel à l'aide du logiciel WinMerge.
Nous avons donc deux fichiers :
    o wikicode_avant_dump.txt => wikicode de l'entrée "avant" extrait du dump
    o wikicode_avant_2019-01-22-16h41.txt => wikicode de la page à cette date

On peut faire les remarques suivantes à propos du wikicode actuel par rapport 
au dump :
    o Le format du wikicode n'a pas évolué.
    o Des prononciations ont été rajoutées dans la section 
      === {{S|prononciation}} ===
    o Des traductions ont été rajoutées dans la section 
      ==== {{S|traductions}} ==== pour la préposition et l'adverbe français.
    o Une nouvelle entrée pour la langue gallo a été rajoutée 
      (langue d'oïl parlée en Bretagne)
    o Des liens de la forme [[code langue:avant]] ont été supprimés à la fin de
      la page.

Le nombre de '=' indique la profondeur de la section dans l'arbre des sections
du document. On relève la structure suivante dans la version à la date du 22 
janvier 2019 à 16h41 :
    o == {{langue|fr}} ==
        o === {{S|étymologie}} ===
        o === {{S|préposition|fr}} ===
            o ==== {{S|composés}} ====
            o ==== {{S|traductions}} ====
        o === {{S|adverbe|fr}} ===
            o ==== {{S|antonymes}} ====
            o ==== {{S|dérivés}} ====
            o ==== {{S|traductions}} ====
                o ===== {{S|traductions à trier}} =====
        o === {{S|nom|fr}} ===
            o ==== {{S|dérivés}} ====
            o ==== {{S|traductions}} ====
        o === {{S|adjectif|fr}} ===
        o === {{S|prononciation}} ===
            o ==== {{S|homophones}} ====
        o === {{S|anagrammes}} ===
        o === {{S|références}} ===
    o == {{langue|oc}} ==
        o === {{S|étymologie}} ===
        o === {{S|adverbe|oc}} ===
            o ==== {{S|synonymes}} ====
            o ==== {{S|antonymes}} ====
            o ==== {{S|dérivés}} ====
        o === {{S|préposition|oc}} ===
            o ==== {{S|synonymes}} ====
            o ==== {{S|antonymes}} ====
        o === {{S|nom|oc}} ===
            o ==== {{S|synonymes}} ====
            o ==== {{S|dérivés}} ====
        o === {{S|références}} ===

4. Analyse et conversion du wikicode [ANA]
==========================================

4.1 Sections, sous-sections et patrons
--------------------------------------

4.1.1 Extraction et trie des types de sections
----------------------------------------------

Pour trouver les types de section existantes dans le dump, nous utilisons les
commandes suivantes :

................................................................................
grep '= {{S|' glawiSplit__1.xml | grep '^=' | grep '=$' | sort > sections.txt
sed -r 's/^=* \{\{S\|//' sections.txt > sections2.txt
sed -r 's/([^\|]*)\|[^|\]+\|[^|\]+\|[^|\]+\}\} ?=+/\1/' sections2.txt | sed -r 's/([^\|]*)\|[^|\]+\|[^|\]+\}\} ?=+/\1/' | sed -r 's/([^\|]*)\|[^|\]+\}\} ?=+/\1/' | sed -r 's/([^\|]*)\}\} ?=+/\1/' | sort | uniq -c | sort -r > types.txt
................................................................................

Nous avons préféré faire des étapes intermédiaires.
Dans sections2.txt on se retrouve avec trois types de lignes :
    o type|xxx|xxx|xxx}} =+
    o type|xxx|xxx}} =+
    o type|xxx}} =+
    o type}} =+
Ces trois types de lignes correspondent à nos trois derniers sed.
Nous avons remarqué des séquences incorrectes :
    o absence d'espace entre l'accolade fermante et le premier espace
    o plusieurs espaces entre l'accolade fermante et le premier espace
Nous avons choisi de prendre en compte la première.
La seconde étant trop mineure pour affecter nos résultats.

On compte les doublons avec l'option -c de la commande uniq.
On fait un dernier sort en reverse pour la liste ordonné par fréquences
descendances. Nous reproduisons ici les 5 premières lignes :
    o 62 464 étymologie
    o 46 102 nom
    o 19 985 prononciation
    o 15 359 références
    o 14 136 synonymes
    o 12 996 traductions
    o 12 861 verbe
    o 10 984 adjectif
    o 10 599 voir aussi
    o 10 440 dérivés
L'ensemble des 167 types (dont certains incorrects) se trouve dans le fichier :
types.txt

4.1.2 Comptes dans l'extrait
----------------------------

Nous comptons d'abord les types contenant ' nom ' dans notre extrait :

...........................................
grep -E ' nom( |$)' types.txt > nb_noms.txt
...........................................

Résultats :
    o 46 102 nom
    o  5 218 nom propre
    o     63 nom de famille
    o      6 nom scientifique
    o      5 nom commun
    o      1 nom 
Les types sont au singulier dans notre fichier de résultats.

Le problème de la cohérence est central dans ses ressources. 
Ainsi nom, nom commun, nom et nom scientifique peuvent être agrégés dans un seul
type, nom commun, pour notre exercice. De même, nom propre et nom de famille
dans le type nom propre. Ce qui nous donne :
    o 46 114 noms communs
    o  5 191 noms propres

Nous comptons ensuite les verbes dans notre extrait :

...............................................
grep -E ' verbe( |$)' types.txt > nb_verbes.txt
...............................................

Résultat :
    o 12 861 verbe

Pour compter le nombre d'infinitif et de formes fléchies, il faut revenir à
notre extrait initial. Nous supposons que :
    o Le niveau 2 est une langue
        o === {{S|verbe|fr|flexion}} === : indique une forme fléchie
        o === {{S|verbe|fr}} === : indique un infinitif
        
Le code de langue étant au minimum sur deux caractères en minuscules, il suffit
donc de passer ces commandes :

................................................................................
grep -E '=== {{S|verbe|[a-z][a-z]+|flexion}} ===' glawiSplit__1.xml > flexions.txt
wc -l flexions.txt
................................................................................

Ce qui nous donne 2 728 516 formes verbales fléchies.

........................................................................
grep -E '=== {{S|verbe|[a-z][a-z]+}} ===' glawiSplit__1.xml > verbes.txt
wc -l verbes.txt
........................................................................

Ce qui nous donne 271 073 verbes à l'infinitif.

4.1.3 Patrons les plus fréquents
--------------------------------

Pour trouver les patrons les plus fréquents en dehors de ceux indiquant une
section de langue ou un titre de section / sous-section, nous procédons ainsi :

................................................................................
grep -E '= {{[^{S]' glawiSplit__1.xml | grep -v -E '= {{langue' > patrons.txt
sed -r 's/[^\{]*\{\{([^\}]*)\}\}.*/\1/' patrons.txt > patrons2.txt
sed -r 's/([^\|]*)\|.*$/\1/' patrons2.txt | grep -v -E '*\{?S$' | sort | uniq -c | sort -r > patrons3.txt
................................................................................

Dans la première ligne, nous sélections toutes les lignes avec un {{ patron }}.
Dans la deuxième ligne, nous ne prenons que l'intérieur des accolades.
Dans la troisième ligne, nous supprimons les arguments du patrons, puis enlevons
les séquences problématiques S et {S avant de trier, de supprimer les doublons
en comptant et enfin de retrier en inverse pour avoir les patrons par ordre 
décroissant de fréquence. Nous avons pour résultats le fichier patrons3.txt,
dont nous prenons les lignes avec une fréquence supérieur ou égale à trois :
    o 625 caractère
    o  35 modl
    o  18 +
    o   8 polytonique
    o   8 fr-conj-1
    o   4 es-conj-2-tener
    o   4 Etymologie graphique chinoise 
    o   3 Étymologie graphique chinoise 
    o   3 es-conj-2-haber
    o   3 de
    o   3 T

Dans notre premier grep, nous ne voulons par des séquences {{S et {{{ car nous
avons remarqué la présence de triplet d'ouverture pour {{{sél, {{{1, {{{nocat
avec la commande suivante :

..................................
grep '{{{' glawiSplit__1.xml | cat
..................................

4.2 Langue des étymons
----------------------

4.2.1 Section étymologie
------------------------

Les étymologies sont dans une section signalée par === {{S|étymologie}} ===
Les lignes de cette section commence toujours par un double point ":".

Voici quelques exemples d'étymologies dans notre extrait autour de l'entrée
avant :

=== {{S|étymologie}} ===
: ''(Préposition et adjectif)'' {{date|842}} Du bas {{étyl|la|fr}} ''[[ab ante#la|ab ante]]'', qui est une forme renforcée de ''[[ante#la|ante]]'' (« avant »).
: ''(Nom)'' {{date|1678}} Même origine. {{date|1422}} ''[[avance]]''.

=== {{S|étymologie}} ===
:De ''[[alternus#la|alternus]]'' avec le suffixe ''[[-e#la|-e]]''.

=== {{S|étymologie}} ===
:{{date}} Mot {{compos|isomère|-ie|lang=fr}}.

4.2.2 Extraction des étymologies
--------------------------------

Pour extraire les étymmologies, nous prenons toutes les lignes commençant par
un double point.

.............................................
grep '^:' glawiSplit__1.xml > etymologies.txt
.............................................

4.2.3 Patron des étymologies
----------------------------

Le patron utilisé est : {{étyl}}.
Il est documenté sur cette page :
    https://fr.wiktionary.org/wiki/Mod%C3%A8le:%C3%A9tyl
    
Sa syntaxe :
    {{étyl|
        code-langue-1|
        code-langue-2|
        mot=mot1|
        tr=translittération1|
        type=code-gramm|
        num=lemme|
        sens=traduction1
    }}

Quelques exemples :
    o {{étyl|la|fr|mot=alternus}} pour alterne
    o {{étyl|la|nl|mot=albumen|sens=blanc d’œuf}} pour albumen

Le code-langue-1 indique la langue d'origine de l'étymon.
Le code-langue-2 indique la langue de l'entrée.
    On peut s'interroger sur le "nl" du deuxième exemple qui pour nous est une
    erreur, il devrait être mis "fr". Cela a été corrigée dans la version en
    ligne d'albumen (au 23 janvier 2019 à 16h29).

4.2.4 Les langues des étymons par fréquence décroissante
--------------------------------------------------------

Pour extraire toutes les utilisations du patron, nous utilisons les commandes
suivantes :

................................................................................
grep "tyl|" glawiSplit__1.xml | sed -r 's/[^\{]*\{\{[^\|]*([^\}]*)\}.*/\1/' > etymologies_patrons.txt
................................................................................

Le fichier etymologies_patrons.txt contient seulement les arguments du patron
car nous avions un problème d'encodage sur le 'é' de étyl. Nous avons contourné
la difficulté dans l'écriture de notre expression régulière. Voici des exemples
tirés du fichier :

|it|fr|mot=lira
|la|fr|mot=interrogatio
|la|fr|mot=procrastino|dif=procrastinare

Nous faisons ensuite une extraction du code-langue-1 avec trie et unicité pour
obtenir une liste des langues d'étymons par ordre décroissant de fréquence :

................................................................................
sed -r 's/^\|([^|]*)\|.*/\1/' etymologies_patrons.txt | sort | uniq -c | sort -r > langues_etymons.txt
................................................................................

Les 10 langues d'origine des étymons les plus fréquentes sont :
   o 8 894 la
   o 1 025 fr
   o   848 grc
   o   619 en
   o   406 it
   o   365 de
   o   337 indo-européen commun
   o   333 es
   o   320 eo
   o   304 goh

4.3 Extraction et conversion des gloses
---------------------------------------

4.3.1 Signalement des gloses
----------------------------

Nous avons étudié l'entrée "tour" car elle contient deux sens :
    o la tour, construction défensive élevée
    o le tour, déplacement qui finit là où il commence
    
Les deux gloses / sens sont traduits par deux sections :
    o === {{S|nom|fr|num=1}} ===
    o === {{S|nom|fr|num=2}} ===

Les gloses sont donc des sections de la forme : 

=== {{S|pos|code-langue|num=nombre}} ===

Le nombre est là pour distinguer entre plusieurs sens de même catégorie du
discours (part of speech).

4.3.2 Extraction et comptage de l'ensemble des gloses
-----------------------------------------------------

Selon la page : 
https://fr.wiktionary.org/wiki/Wiktionnaire:Liste_de_tous_les_mod%C3%A8les/Titres_de_sections/Liste_automatique

Il existe 67 catégories du discours différentes, certaines utilisant le même
code. Nous avons fait le fichier pos.txt qui reprend ces catégories par un 
copier-coller de la page. Pour que cela marche avec cut et uniq, avec Notepad++,
nous convertissons les tabulations en espace et les fins de ligne en UNIX.

Nous exécutons ensuite la commande suivante :

....................................................
cut -d ' ' -f 1 pos.txt | sort | uniq > pos_uniq.txt
....................................................

Pour obtenir dans le fichier pos_uniq.txt juste les codes uniques. 

Puis, par concaténation manuelle nous fabriquons la commande suivante :

................................................................................
grep -E "{{S\|adj|{{S\|adj-dém|{{S\|adj-excl|{{S\|adj-indéf|{{S\|adj-int|{{S\|adj-num|{{S\|adj-pos|{{S\|adj-rel|{{S\|adjectif|{{S\|adv|{{S\|adv-ind|{{S\|adv-int|{{S\|adv-pron|{{S\|adv-rel|{{S\|adverbe|{{S\|aff|{{S\|art|{{S\|art-déf|{{S\|art-indéf|{{S\|art-part|{{S\|article|{{S\|circon|{{S\|circonf|{{S\|class|{{S\|classif|{{S\|conj|{{S\|conj-coord|{{S\|conjonction|{{S\|copule|{{S\|dét|{{S\|encl|{{S\|faute|{{S\|gismu|{{S\|inf|{{S\|interf|{{S\|interj|{{S\|lettre|{{S\|loc|{{S\|loc-phr|{{S\|locution|{{S\|locution-phrase|{{S\|nom|{{S\|nom-fam|{{S\|nom-pr|{{S\|nom-sciences|{{S\|num|{{S\|numeral|{{S\|numér|{{S\|onom|{{S\|onoma|{{S\|part|{{S\|part-num|{{S\|particule|{{S\|patronyme|{{S\|phr|{{S\|phrase|{{S\|post|{{S\|postpos|{{S\|procl|{{S\|pronom|{{S\|pronom-adj|{{S\|pronom-dém|{{S\|pronom-indéf|{{S\|pronom-int|{{S\|pronom-per|{{S\|pronom-pers|{{S\|pronom-pos|{{S\|pronom-rel|{{S\|pronom-réfl|{{S\|prov|{{S\|pré-nom|{{S\|pré-verb|{{S\|préf|{{S\|prénom|{{S\|prép|{{S\|quantif|{{S\|rad|{{S\|radical|{{S\|rafsi|{{S\|sino|{{S\|sinog|{{S\|sinogramme|{{S\|substantif|{{S\|suf|{{S\|suff|{{S\|symb|{{S\|var-typo|{{S\|variante|{{S\|verb" glawiSplit__1.xml > gloses1.txt
................................................................................

Nous avons ainsi toutes les patrons indiquant une glose avec ses paramètres.

Pour faire un compte, il suffit d'effectuer :

................................................................................
cut -d '|' -f 2 gloses1.txt | sed -r 's/^([a-z]*).*/\1/' | sort | uniq -c | sort -r > gloses_count.txt
................................................................................

Les 5 premières lignes du fichier gloses_count.txt sont :
    o 52 313 nom
    o 13 137 verbe
    o 12 050 adjectif
    o  3 608 adverbe
    o  2 857 variantes

4.3.3 Encodage des hyperliens
-----------------------------

Les hyperliens sont encodés en wikicode ainsi : [[ ]]

Ils peuvent eux aussi avec des arguments à l'intérieur, séparés par |

Les liens vers des fichiers ont "Fichier:" comme préfixe :

[[Fichier:Tour de Montlhéry.jpg|vignette|La '''tour''' de Montlhéry ''(sens 1)'']]

................................................................................
sed -r 's/\[\[(Fichier|Image):.*\]\]//' glawiSplit__1.xml | sed -r 's/\[\[[^\:]*:(.*)\]\]/\1/g' | sed -r 's/\[\[([^\|]*).*\]\]/\1/' | sed -r 's/\[\[//' | sed -r 's/\]\]//' > nohyperlink.txt
................................................................................

Notes :
    o Le premier sed remplace les liens de la forme "[[Fichier:...]" ou ceux
      de la forme "[[Image:...]]" par rien.
    o Le deuxième sed remplace les liens de la forme [[langue:mot]] par mot.
    o Le troisième sed remplace les liens de la forme [[xxx|...]] par xxx.
    o Si sur une même ligne on a "[[xxx]] ou [[yyy]]" le troisième sed donne
      "xxx]]" ou "[[yyy". Pour effacer les dernières traces on fait deux simples
      sed sur "]]" et "[[".
    o Théoriquement, en rendant l'opérateur * non greedy avec ?, cela devrait
      éviter de faire les deux derniers sed, mais sur notre environnement, cela
      ne marche pas. Après bien des interrogations, nous avons essayé l'exemple
      suivant donné dans le cours :
      ..............................................................
      echo "un lapin au vin blanc" | sed -r 's/l.*?n/DICTIONNAIRE/g'
      ..............................................................
      Malgré l'opérateur ? modificateur du * pour le rendre non greedy
      et le modificateur global g pour effectuer le remplacement autant de fois 
      que le motif est trouvé, nous obtenons : un DICTIONNAIREc au lieu de :
      un DICTIONNAIRE au vin bDICTIONNAIREc
    o Nous avons également testé avec le même résultat ici (GNU Sed 4.4) :
      https://www.tutorialspoint.com/execute_bash_online.php

Une solution serait d'utiliser Perl pour palier le problème de .*? :

................................................................................
perl -CSDA -p -e 's/\[\[(Fichier|Image):.*\]\]//' glawiSplit__1.xml | perl -CDSA  -p -e 's/\[\[[^\:]*:(.*)\]\]/\1/g' | perl -CSDA -p -e 's/\[\[([^#]*?)\#.*?\]\]/\1/g' | perl -CSDA -p -e 's/\[\[([^\|]*?)\|.*?\]\]/\1/g' | perl -CSDA -p -e 's/\[\[(.*?)\]\]/\1/g' | perl -CDSA -p -e 's/(\[\[)|(\]\])//g' > nohyperlink2.txt
................................................................................

Cette dernière commande permet de corriger de nombreux problèmes de la
précédente.

4.3.4 Encodage du gras et de l'italique
---------------------------------------

Le gras est encodé par '''mot en gras'''.
L'italique est encodé par ''mot en italique''.

On fait l'opération suivante :

...................................................
sed -r "s/'''?//g" glawiSplit__1.xml > noquotes.txt
...................................................

Pour encoder le formatage au lieu de le supprimer :

................................................................................
sed -r "s/'''(.*?)'''/<b>\1<\/b>/g" glawiSplit__1.xml| sed -r "s/''(.*?)''/<i>\1<\/i>/g" > noquotes.txt
................................................................................

Notes : comme expliqué plus haut, utiliser le ? pour rendre le * non greedy ne
marche pas sur ma configuration. Il le faut car sinon :
''aaa'' ''bbb'' est transformé en <i>aaa'' ''bbb</i> car .* "mange" le plus
possible. Alors qu'avec .?*, il s'arrête au premier '' qu'il rencontre ce qui
est essentiel pour différencier un '' fermant d'un '' ouvrant pour remplacer par
<i> ou <\i>.

Une solution est d'utiliser Perl avec :
................................................................................
perl -p -e "s/'''(.*?)'''/<b>\1<\/b>/g" glawiSplit__1.xml | perl -p -e "s/''(.*?)''/<i>\1<\/i>/g" > quotebold.txt
................................................................................

Mais nous avons un autre problème : la succession de deux balises wikicode comme
dans l'exemple :

#* '''''Pendant''' de baudrier ou de ceinturon.''

La bonne traduction est <i><b>Pendant</b> de baudrier ou de ceinturon.</i>
Alors que la nôtre est <b><i>Pendant</b> de baudrier ou de ceinturon.</i>
Ce qui ne respecte pas les règles XHTML (HTML est lui permissif) :
On ne doit pas fermer une balise parente puis fermer une balise fille.
Cela vient du fait que nous matchons en premier le triple. On corrige ainsi
notre commande :

................................................................................
perl -CSDA -p -e "s/'''([\w ].*?)'''/<b>\1<\/b>/g" glawiSplit__1.xml | perl -p -e "s/''(.*?)''/<i>\1<\/i>/g" > quotebold.txt
................................................................................

Nous obtenons la bonne traduction. Nous précisons CSDA pour signaler à Perl de
fonctionner en Unicode pour que \w reconnaisse bien toutes les lettres non
romanes.

4.3.5 Les 10 patrons qui restent
--------------------------------

Nous cumulons les deux traitements précédents pour obtenir un fichier sans
liens et en ayant converti le gras et l'italique.

................................................................................
perl -CSDA -p -e 's/\[\[(Fichier|Image):.*\]\]//' glawiSplit__1.xml | perl -CDSA  -p -e 's/\[\[[^\:]*:(.*)\]\]/\1/g' | perl -CSDA -p -e 's/\[\[([^#]*?)\#.*?\]\]/\1/g' | perl -CSDA -p -e 's/\[\[([^\|]*?)\|.*?\]\]/\1/g' | perl -CSDA -p -e 's/\[\[(.*?)\]\]/\1/g' | perl -CDSA -p -e 's/(\[\[)|(\]\])//g' | perl -CSDA -p -e "s/'''([\w ].*?)'''/<b>\1<\/b>/g" | perl -p -e "s/''(.*?)''/<i>\1<\/i>/g" > nothing.txt
................................................................................

Il faut à présent sélectionner les patrons en enlevant tous les patrons traitant
des types de mots déjà analysés :

................................................................................
grep -E "{{" nothing.txt | grep -v -E "{{S\|adj|{{S\|adj-dém|{{S\|adj-excl|{{S\|adj-indéf|{{S\|adj-int|{{S\|adj-num|{{S\|adj-pos|{{S\|adj-rel|{{S\|adjectif|{{S\|adv|{{S\|adv-ind|{{S\|adv-int|{{S\|adv-pron|{{S\|adv-rel|{{S\|adverbe|{{S\|aff|{{S\|art|{{S\|art-déf|{{S\|art-indéf|{{S\|art-part|{{S\|article|{{S\|circon|{{S\|circonf|{{S\|class|{{S\|classif|{{S\|conj|{{S\|conj-coord|{{S\|conjonction|{{S\|copule|{{S\|dét|{{S\|encl|{{S\|faute|{{S\|gismu|{{S\|inf|{{S\|interf|{{S\|interj|{{S\|lettre|{{S\|loc|{{S\|loc-phr|{{S\|locution|{{S\|locution-phrase|{{S\|nom|{{S\|nom-fam|{{S\|nom-pr|{{S\|nom-sciences|{{S\|num|{{S\|numeral|{{S\|numér|{{S\|onom|{{S\|onoma|{{S\|part|{{S\|part-num|{{S\|particule|{{S\|patronyme|{{S\|phr|{{S\|phrase|{{S\|post|{{S\|postpos|{{S\|procl|{{S\|pronom|{{S\|pronom-adj|{{S\|pronom-dém|{{S\|pronom-indéf|{{S\|pronom-int|{{S\|pronom-per|{{S\|pronom-pers|{{S\|pronom-pos|{{S\|pronom-rel|{{S\|pronom-réfl|{{S\|prov|{{S\|pré-nom|{{S\|pré-verb|{{S\|préf|{{S\|prénom|{{S\|prép|{{S\|quantif|{{S\|rad|{{S\|radical|{{S\|rafsi|{{S\|sino|{{S\|sinog|{{S\|sinogramme|{{S\|substantif|{{S\|suf|{{S\|suff|{{S\|symb|{{S\|var-typo|{{S\|variante|{{S\|verb" > nothing2.txt
................................................................................
 
A partir du fichier nothing2.txt obtenu, nous analysons les patrons restant :

................................................................................
perl -CSDA -p -e 's/(.*?)\{\{(.*?)\}\}(.*?)/\2/g' nothing2.txt > nothing3.txt

sort nothing3.txt | uniq -c | sort -r > nothing4.txt

head -n 10 nothing4.txt > last_patrons.txt
................................................................................

Voici les 10 patrons les plus fréquents restant :
    o 62 456 S|étymologie ===
    o 19 976 S|prononciation ===
    o 17 225 langue|fr ==
    o 16 420 trad-fin
    o 15 273 S|références ===
    o 14 115 S|synonymes ====
    o 12 881 S|traductions ====
    o 10 589 S|voir aussi ===
    o 10 386 S|dérivés ====
    o  9 129 trad-début
J'ai laissé les "=" finaux pour analyser le niveau de titre.

Nous avons observés ces dix patrons à l'aide de grep par exemple pour trois
d'entre eux :

.............................................
grep -E "\{\{S\|étymologie" glawiSplit__1.xml
grep -E trad-fin avant.txt | head -n 10
grep -E trad-début avant.txt | head -n 10
.............................................

À partir de ces observations, nous avons construit des commandes pour remplacer
les 10 patrons les plus fréquents pour rendre plus lisible le Wikicode.

Nous avons notamment choisi de traduire :
{{trad-début|Composés chimique en chimie (1)}}
    xxx
{{trad-fin}}
Par :
<p><u>Composés chimique en chimie (1)</u> :
    xxx
</p>

À chaque ligne suivante correspond un changement de patron :

................................................................................
perl -CSDA -p -e 's/=== \{\{S\|.tymologie\}\} ===/<h3>Etymologie<\/h3>/g' glawiSplit__1.xml | \
perl -CSDA -p -e 's/=== \{\{S\|prononciation\}\} ===/<h3>Prononciation<\/h3>/g' | \
perl -CSDA -p -e 's/=== \{\{S\|r.f.rences\}\} ===/<h3>References<\/h3>/g' | \
perl -CSDA -p -e 's/=== \{\{S\|synonymes\}\} ===/<h3>Synonymes<\/h3>/g' | \
perl -CSDA -p -e 's/=== \{\{S\|traductions\}\} ===/<h3>Traductions<\/h3>/g' | \
perl -CSDA -p -e 's/=== \{\{S\|voir aussi\}\} ===/<h3>Voir aussi<\/h3>/g' | \
perl -CSDA -p -e 's/=== \{\{S\|d.riv.s\}\} ===/<h3>Derives<\/h3>/g' | \
perl -CSDA -p -e 's/\{\{trad-d.but\|(.*?)\}\}/<p><u>\1<\/u> :/g' | \
perl -CSDA -p -e 's/\{\{trad-fin\}\}/<\/p>/g'> last10.txt
................................................................................

Nous avons dû utiliser le "." à chaque caractère accentué, ayant des problèmes
d'encodage.

5. GLÀFF [GLÀFF]
================

5.1 Nombre de formes, nombre de lemmes
--------------------------------------

Nous décompressons le lexique à partir de l'archive GLÀFF-1.2.1.tar.bz.
Nous obtenons le fichier GLAFF-1.2.2.txt.

Pour mettre au point nos scripts, nous créons miniGLÀFF.txt avec la commande :

............................................
head -n 1000 GLAFF-1.2.2.txt > miniGLÀFF.txt
............................................

Exemple de ligne :
affluent|Vmip3p-|affluer|a.fly|a.fly|9|0.318|187|6.485|369|1.672|1207|5.498|500|0.393|1929|1.538

Pour compter le nombre de formes :

.....................
wc -l GLAFF-1.2.2.txt
.....................

On a 1 406 857 formes.

Pour compter le nombre de lemmes :

.....................................................
cut -d "|" -f 3 GLAFF-1.2.2.txt | sort | uniq | wc -l
.....................................................

On prend la colonne n°3 (en comptant à partir de un) qui contient le lemme, on
trie puis on applique uniq et on compte le nombre de ligne. On a 164 291 lemmes.

5.2 Trie des étiquettes morphosyntaxiques par fréquence décroissante
--------------------------------------------------------------------

L'étiquette est dans la deuxième colonne (en comptant à partir de un). On
effectue donc la commande suivante :

...........................................................................
cut -d "|" -f 2 GLAFF-1.2.2.txt | sort | uniq -c | sort -r > etiquettes.txt
...........................................................................

Les 10 étiquettes les plus fréquentes sont les 10 premières lignes du fichier.
Le format utilisé pour les étiquettes est GRACE. 
Sa documentation est accessible sur ce site :
http://redac.univ-tlse2.fr/GLÀFFoli/about/posGrace.jsp

Nous présentons ici les 10 étiquettes les plus fréquentes avec la signification
du code GRACE :
    o 54 226 Ncfs    Nom commun féminin singulier
    o 50 532 Ncms    Nom commun masculin singulier
    o 45 223 Ncfp    Nom commun féminin pluriel
    o 42 406 Ncmp    Nom commun masculin pluriel
    o 39 696 Afpfs   Adjectif qualificatif féminin singulier
    o 36 652 Afpfp   Ajdectif qualificatif féminin pluriel
    o 25 416 Afpms   Adjectif qualificatif masculin singulier
    o 23 646 Afpmp   Adjectif qualificatif masculin pluriel
    o 21 801 Vmip3s- Verbe sous forme flexionnelle à l'indicatif 3e personne
                     du singulier
    o 21 798 Vmcp3s- Verbe sous forme flexionnelle au conditionnel 3e personne
                     du singulier

5.3 Formes fléchis du verbe googler
-----------------------------------

Une étiquette GRACE commençant par Vmn est un verbe à l'infinitif.
Nous ne voulons que les formes fléchies du verbe googler.

Première étape, filtrer pour n'avoir que le lemme googler :

......................................................
grep -E "V......\|googler\|" GLAFF-1.2.2.txt > tmp.txt
......................................................

Seconde étape, filtrer pour n'avoir que les formes fléchies et pas l'infinitif :

...........................................
grep -v -E "\|Vmn----\|" tmp.txt > tmp2.txt
...........................................

Dernière étape, couper pour n'avoir que la première colonne :

............................................
cut -d "|" -f 1 tmp2.txt | sort > google.txt
............................................

Et on obtient toutes les formes fléchies du verbe googler sans son infinitif.
Il y en a 50 (avec wc -l).
En n'ayant que la forme, nous avons donc des doublons et pour les supprimer nous
pouvons faire :

...............
uniq google.txt
...............

Ce qui ramène à 38 formes et nous amène à réfléchir sur la notion de forme :

    Une forme fléchie peut être comprise simplement comme une suite de lettre :
        parle = parle
    Mais une forme fléchie peut être aussi comprise comme une forme ET une
    catégorisation. Ainsi dans cette optique :
        parle|Vmip1s- (indicatif, présent, 1ère personne, singulier)
        parle|Vmmp2s- (impératif, présent, 2ème personne, singulier)
    ne sont PAS la même forme, alors que dans la première optique, si.
    Nous préciserons à chaque fois dans quelle optique nous nous plaçons.

5.4 Formes fléchis avec étiquettes morphosyntaxiques du verbe hacker
--------------------------------------------------------------------

Dans la question précédente, nous avons compris la mention :
    formes fléchies (uniquement) 
comme le fait de ne pas prendre l'infinitif.

Pour les questions suivantes, la mention formes fléchies n'est pas suivie de
uniquement, nous pensons donc qu'il faut prendre également l'infinitif.

En effet, l'infinitif est une forme fléchie, avec le suffixe -er, qui a la
particularité d'avoir été choisie pour représenter le lemme, c'est sa forme
canonique ou de citation.

Nous prenons donc dans les questions suivantes l'infintif également en 
considération, sachant que sa suppression pas simplement par l'ajout du filtre
suivant :

........................
grep -v -E "\|Vmn----\|"
........................

Pour toutes les formes fléchies du verbe hacker, nous reprenons les étapes pour
googler en gardant l'infinitif et en changeant juste la dernière étape pour
garder également l'étiquette :

............................................................................
grep -E "Vm.....\|hacker\|" GLAFF-1.2.2.txt | cut -d "|" -f 1,2 > hacker.txt
............................................................................

Comme il nous est demandé la forme fléchie et l'étiquette, nous nous plaçons
dans cette optique. Le fichier résultat a 51 formes. Par rapport aux 50 de
googler dans la même optique, la différence est l'infinitif.

5.5 Nombre de formes fléchies pour chaque verbe
-----------------------------------------------

Première chose, nous allons prendre que les verbes, avec les infinitifs :

................................................
grep -E "\|V......\|" GLAFF-1.2.2.txt > tmp1.txt
................................................

Puis nous allons garder seulement le lemme car nous voulons simplement compter.
En effet, si on a :
    o dansons|...|danser
    o dansez|...|danser
On aura :
    o danser
    o danser
Donc on sait qu'on a deux formes fléchies pour danser mais on ne sait pas
lesquelles. Il suffit ensuite faire sort, uniq -c puis sort -r.

..................................................................
cut -d "|" -f 3 tmp1.txt | sort | uniq -c | sort -r > flechies.txt
..................................................................

Nous voulons ensuite savoir combien il y a de nombres de formes fléchies, en
considérant donc qu'une forme et à la fois une suite de lettres et une
catégorisation. Pour cela, nous faisons la commande suivante :

.............................................................................
sed -r 's/[^0-9]+([0-9]+).*/\1/' flechies.txt | uniq -c | sort -r > count.txt
.............................................................................

Les 13 premiers résultats sont :
    o 17 003 verbes ont 51 formes fléchies
    o  2 789 verbes ont 48 formes fléchies
    o    505 verbes ont 50 formes fléchies
    o    353 verbes ont 72 formes fléchies
    o    120 verbes ont 47 formes fléchies
    o    102 verbes ont  1 formes fléchies
    o     92 verbes ont 52 formes fléchies
    o     85 verbes ont 49 formes fléchies
    o     75 verbes ont 69 formes fléchies
    o     60 verbes ont 53 formes fléchies
    o     26 verbes ont 71 formes fléchies
    o     26 verbes ont 10 formes fléchies
    o     12 verbes ont 98 formes fléchies
    o     12 verbes ont 60 formes fléchies

Le minimum de formes fléchies est 1 pour 102 verbes.
Le maximum de formes fléchies est 119 pour 1 verbes (broebeler).
La moyenne est de 51 formes fléchies par verbe.
79% des verbes ont 51 formes fléchies. 
13% des verbes ont 48 formes fléchies.
En comptant les verbes de 51 et 48 formes, on obtient 92%.

Les verbes avec très peu de formes fléchis s'expliquent car le matériau de base
à partir duquel a été constitué GLÀFF peut ne pas contenir toutes les formes
fléchies des verbes. On a par exemple :
    fazéier, embaquer, dérompre, déradicaliser, décarpiller, débocarder,
    débadger, câlicer, cassecouiller, carapaçonner, bréquer, brasseïer
    bamboucher

Les verbes surabondants peuvent s'expliquer par une possibilité de choix dans
la racine qui permet ensuite d'avoir toutes les formes fléchies en double.
Nous avons étudié le verbe bunkeriser et avons vu que la plupart de ses formes
étaient doublées, sans que ce soit systématique, ainsi :
    o bunkériserions    Vmcp1p-
    o bunkeriserions    Vmcp1p-

5.6 Verbes surabondants et verbes défectifs
-------------------------------------------

Un verbe surabondant à plusieurs formes possibles pour une même personne, un
même genre, un même temps, un même mode.
Un verbe défectif à des formes manquantes.
En nous basant sur les résultats de la partie précédente, nous prenons 51 comme
nombre normal de formes fléchies pour un verbe, en considérant une forme comme
une suite de lettres et une catégorisation.

Il suffit de reprendre le fichier flechies.txt et de prendre toutes les lignes
différentes de 51. Avec un nombre supérieur pour les verbes surabondant, avec
un nombre inférieur pour les verbes défectifs. Ce qui donne pour les 
surabondants :

................................................................................
grep -E "(72)|(52)|(69)|(53)|(71)|(98)|(60)|(55)|(54)|(68)|(81)|(56)|(93)|(73)|(64)|(85)|(70)|(57)|(97)|(74)|(59)|(89)|(82)|(78)|(77)|(66)|(63)|(62)|(61)|(58)|(199)" flechies.txt > surabondants.txt
................................................................................

Et pour les défectifs on fait l'inverse en oubliant pas d'ajouter 51 :

................................................................................
grep -v -E "(51)|(72)|(52)|(69)|(53)|(71)|(98)|(60)|(55)|(54)|(68)|(81)|(56)|(93)|(73)|(64)|(85)|(70)|(57)|(97)|(74)|(59)|(89)|(82)|(78)|(77)|(66)|(63)|(62)|(61)|(58)|(199)" flechies.txt > defectifs.txt
................................................................................

On a :
    o wc -l surabondants.txt =>          699
    o wc -l defectifs.txt    =>        3 708
    ----------------------------------------
    o verbes avec 51 formes fléchies  17 003
    o wc -l flechies.txt     =>       21 410
Le compte est bon.

6. Comparaison des nomenclatures de GLÀFF et de Morphalou [MORPH]
=================================================================

Pour les deux points suivants, nous construisons un fichier de toutes les
formes du Morphalou. Nous avons repéré qu'elles sont dans :
<orthography>mousse</orthography>

................................................................................
grep -E "<orthography>" Morphalou-2.0.xml | sed -r 's/^[^<]*<orthography>([^<]*)<\/orthography>.*$/\1/' | sort | uniq > morphalou.txt
................................................................................

wc -l nous informe qu'il comporte 410 441 formes.

Nous construisons un fichier des formes, en ne considérant que la suite de
lettres cette fois-ci, à partir du GLÀFF :

.........................................................
cut -d "|" -f 1 glaff-1.2.2.txt | sort | uniq > glaff.txt
.........................................................

wc -l nous informe qu'il comporte 1 082 688 formes.

Nous faisons ensuite un diff des deux :

.......................................
diff morphalou.txt glaff.txt > diff.txt
.......................................

Nous avons à présent un fichier à partir duquel travailler.

6.1 Liste des formes dans le Morphalou et pas dans GLÀFF
--------------------------------------------------------

Les lignes du fichier diff.txt commençant par < indique une forme présente dans
le Morphalou et pas dans GLÀFF :

..........................................
grep -E "^<" diff.txt > pas_dans_glaff.txt
..........................................

wc -l nous informe qu'il y en 58 168.

6.2 Liste des formes dans GLÀFF et pas dans Morphalou
-----------------------------------------------------

Les lignes du fichier diff.txt commençant par > indique une forme présente dans
le GLÀFF et pas dans Morphalou :

..............................................
grep -E "^>" diff.txt > pas_dans_morphalou.txt
..............................................

wc -l nous informe qu'il y en 730 415.

+----------------------------------------------+------------+-----------+
|                                              |   GLÀFF    | Morphalou |
+----------------------------------------------+------------+-----------+
| Nombre de forme (suite de lettres seulement) | 1 082 688  |  410 441  |
+----------------------------------------------+------------+-----------+
| Présente uniquement dans la ressource        |   730 415  |   58 168  |
+----------------------------------------------+------------+-----------+
| Pourcentage de non recouvrement              |        67% |       14% |
+----------------------------------------------+------------+-----------+

On constate donc que le GLÀFF est beaucoup plus riche que le Morphalou, mais
qu'il gagnerait à être enrichi des 58 168 formes que le Morphalou possède et pas
lui.

7. Conversion de GLÀFF en format CSV [CSV]
==========================================

7.1 Conversion du GLÀFF en CSV
------------------------------

Nous devons produire une version de GLÀFF qui contienne :
    o forme
    o étiquette
    o lemme
    o transcriptions API (séparées par un ;)

Nous sélectionnons les champs demandés avec la commande suivante :

.............................................
cut -d "|" f 1,2,3,4 miniglawi.txt > tmp1.txt
.............................................

Puis nous remplaçons le | par un , :

......................................
sed -r 's/\|/,/g' tmp1.txt > final.csv
......................................

Et nous pouvons ensuite ouvrir final.csv avec OpenOffice, LibreOffice ou Excel.

================================================================================
Partie B : utilisation de Java [JAVA]
================================================================================

1. Environnement de travail
---------------------------

Pour cette partie, nous développons sur Windows avec un JDK et Notepad++.

Nous utilisons quotidiennement Eclipse à notre travail mais préférons un
environnement plus léger quand c'est possible.

Nous utilisons le script bat suivant pour compiler :

@echo off
set JAVADIR=C:\tools\jdk-11.0.2\bin
echo ------------------- CLEANING ---------------------
del .\saxo\DGXHandler.class
echo ------------------- END CLEANING -----------------
echo ------------------ COMPILING ---------------------
%JAVADIR%\javac.exe .\saxo\DGXHandler.java -Xlint
echo ------------------ END COMPILING ---------------------
pause
if exist .\saxo\DGXHandler.java (
    echo ------------------ EXECUTING ---------------------
    %JAVADIR%\java.exe saxo.DGXHandler
    echo ------------------ END EXECUTING ---------------------
    pause
)
if not exist .\saxo\DGXHandler.java (
    echo SOMETHING WENT WRONT WHILE COMPILING
)

